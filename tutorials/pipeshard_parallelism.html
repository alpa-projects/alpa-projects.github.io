<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Distributed Training with Both Shard and Pipeline Parallelism &mdash; Alpa 0.1.4 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/alpa-logo.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Differences between alpa.parallelize and jax.pmap" href="alpa_vs_pmap.html" />
    <link rel="prev" title="Alpa Quickstart" href="quickstart.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Alpa
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install Alpa</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Alpa Quickstart</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Distributed Training with Both Shard and Pipeline Parallelism</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#import-libraries-and-initialize-environment">Import Libraries and Initialize Environment</a></li>
<li class="toctree-l2"><a class="reference internal" href="#connect-to-a-ray-cluster">Connect to a Ray Cluster</a></li>
<li class="toctree-l2"><a class="reference internal" href="#train-an-mlp-on-a-single-device">Train an MLP on a Single Device</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pipeline-parallelism-with-manual-assignment">Pipeline Parallelism with Manual Assignment</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pipeline-parallelism-with-automatic-assignment">Pipeline Parallelism with Automatic Assignment</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="alpa_vs_pmap.html">Differences between alpa.parallelize and jax.pmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="perf_tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="opt_serving.html">Serving OPT-175B using Alpa</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Architecture</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture/overview.html">Design and Architecture</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../developer/developer_guide.html">Developer Guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Alpa</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Distributed Training with Both Shard and Pipeline Parallelism</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/alpa-projects/alpa/blob/main/docs/tutorials/pipeshard_parallelism.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-tutorials-pipeshard-parallelism-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="distributed-training-with-both-shard-and-pipeline-parallelism">
<span id="sphx-glr-tutorials-pipeshard-parallelism-py"></span><h1>Distributed Training with Both Shard and Pipeline Parallelism<a class="headerlink" href="#distributed-training-with-both-shard-and-pipeline-parallelism" title="Permalink to this headline"></a></h1>
<p>Alpa can automatically parallelizes jax functions with both shard
parallelism (a.k.a. intra-operator parallelism) and pipeline parallelism
(a.k.a. inter-operator parallelism). Shard parallelism includes
data parallelism, operator parallelism, and their combinations.
The <a class="reference internal" href="quickstart.html#alpa-quickstart"><span class="std std-ref">quick start</span></a> focuses on using Alpa for shard parallelism.</p>
<p>In this tutorial, we show how to use Alpa with both shard and pipeline parallelism.
First, we show how to use Alpa to manually assign stages for pipeline parallelism.
Then we show how to use Alpa to automate this process.</p>
<section id="import-libraries-and-initialize-environment">
<h2>Import Libraries and Initialize Environment<a class="headerlink" href="#import-libraries-and-initialize-environment" title="Permalink to this headline"></a></h2>
<p>First, import the required libraries.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">alpa</span>
<span class="kn">from</span> <span class="nn">alpa.testing</span> <span class="kn">import</span> <span class="n">assert_allclose</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">from</span> <span class="nn">flax</span> <span class="kn">import</span> <span class="n">linen</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">flax.training.train_state</span> <span class="kn">import</span> <span class="n">TrainState</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span> <span class="nn">optax</span>
<span class="kn">import</span> <span class="nn">ray</span>
</pre></div>
</div>
</section>
<section id="connect-to-a-ray-cluster">
<h2>Connect to a Ray Cluster<a class="headerlink" href="#connect-to-a-ray-cluster" title="Permalink to this headline"></a></h2>
<p>Alpa uses a distributed framework <a class="reference external" href="https://docs.ray.io/">ray</a> to manage
the cluster and disributed workers. We initialize ray and alpa.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">alpa</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">cluster</span><span class="o">=</span><span class="s2">&quot;ray&quot;</span><span class="p">)</span>

<span class="c1"># Alternatively, you can use the following command to connect to an existing</span>
<span class="c1"># ray cluster.</span>
<span class="c1"># ray.init(address=&quot;auto&quot;)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>2022-07-01 23:11:40,545 INFO services.py:1333 -- View the Ray dashboard at http://127.0.0.1:8265
</pre></div>
</div>
</section>
<section id="train-an-mlp-on-a-single-device">
<h2>Train an MLP on a Single Device<a class="headerlink" href="#train-an-mlp-on-a-single-device" title="Permalink to this headline"></a></h2>
<p>In this tutorial, we use a toy dataset to train an MLP model.
Specifically, we use the model to fit the function: <span class="math notranslate nohighlight">\(y = Wx + b\)</span>.
Note that now this model is being executed on CPU because we force the driver
process to use the CPU.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MLPModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span>

    <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dim</span></a> <span class="o">=</span> <span class="mi">2048</span>
<a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a> <span class="o">=</span> <span class="mi">2048</span>

<span class="c1"># Generate ground truth W and b</span>
<span class="n">rngkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">k1</span><span class="p">,</span> <span class="n">k2</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rngkey</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k1</span><span class="p">,</span> <span class="p">(</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dim</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dim</span></a><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k2</span><span class="p">,</span> <span class="p">(</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dim</span></a><span class="p">,),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Generate the training data</span>
<span class="n">ksample</span><span class="p">,</span> <span class="n">knoise</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">k1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">ksample</span><span class="p">,</span> <span class="p">(</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dim</span></a><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">W</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">knoise</span><span class="p">,</span> <span class="p">(</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch_size</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dim</span></a><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Initialize a train state, which includes the model paramter and optimizer</span>
<span class="c1"># state.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLPModel</span><span class="p">(</span><span class="n">hidden_dim</span><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dim</span></a><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">rngkey</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">tx</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">TrainState</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">apply_fn</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">apply</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">tx</span><span class="o">=</span><span class="n">tx</span><span class="p">)</span>


<span class="c1"># Define the training step</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch</span></a><span class="p">):</span>

    <span class="k">def</span> <span class="nf">loss_func</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch</span></a><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">])</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">out</span> <span class="o">-</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch</span></a><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_func</span><span class="p">)(</span><span class="n">state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">new_state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads</span><span class="o">=</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_state</span>


<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch</span></a> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">y</span><span class="p">}</span>
<span class="n">expected_state</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch</span></a><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="pipeline-parallelism-with-manual-assignment">
<h2>Pipeline Parallelism with Manual Assignment<a class="headerlink" href="#pipeline-parallelism-with-manual-assignment" title="Permalink to this headline"></a></h2>
<p>Pipeline paralleism requires partitioning the model into several pipeline
stages. To manually assign stages, we can use <code class="docutils literal notranslate"><span class="pre">alpa.mark_pipeline_boundary</span></code>
to mark the boundary of each pipeline stage in the forward function.
Note that each pipeline stage is also automatically parallelized by the
shard parallel pass.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a MLP model with manual stage boundaries.</span>
<span class="k">class</span> <span class="nc">ManualPipelineMLPModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span>

    <span class="nd">@nn</span><span class="o">.</span><span class="n">compact</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Use this boundary marker to separate the model into two stages.</span>
        <span class="n">alpa</span><span class="o">.</span><span class="n">mark_pipeline_boundary</span><span class="p">()</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="c1"># Initialize the train state with the same parameters as the single-device</span>
<span class="c1"># model.</span>
<span class="n">manual_pipeline_model</span> <span class="o">=</span> <span class="n">ManualPipelineMLPModel</span><span class="p">(</span><span class="n">hidden_dim</span><span class="o">=</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dim</span></a><span class="p">)</span>
<span class="n">manual_pipeline_state</span> <span class="o">=</span> <span class="n">TrainState</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">apply_fn</span><span class="o">=</span><span class="n">manual_pipeline_model</span><span class="o">.</span><span class="n">apply</span><span class="p">,</span>
                                          <span class="n">params</span><span class="o">=</span><a href="https://docs.python.org/3/library/copy.html#copy.deepcopy" title="copy.deepcopy" class="sphx-glr-backref-module-copy sphx-glr-backref-type-py-function"><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span></a><span class="p">(</span><span class="n">params</span><span class="p">),</span>
                                          <span class="n">tx</span><span class="o">=</span><span class="n">tx</span><span class="p">)</span>


<span class="c1"># Define the training step.</span>
<span class="c1"># We use the &quot;alpa.PipeshardParallel&quot; option to let alpa use both</span>
<span class="c1"># pipeline parallelism and shard parallelism. To make pipeline parallelism</span>
<span class="c1"># efficient, we need to fill the pipeline with many micro batches,</span>
<span class="c1"># so a `num_micro_batches` should be specified.</span>
<span class="nd">@alpa</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><a href="https://docs.python.org/3/library/abc.html#abc.ABC" title="abc.ABC" class="sphx-glr-backref-module-abc sphx-glr-backref-type-py-class"><span class="n">alpa</span><span class="o">.</span><span class="n">PipeshardParallel</span></a><span class="p">(</span><span class="n">num_micro_batches</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                                                <span class="n">layer_option</span><span class="o">=</span><span class="s2">&quot;manual&quot;</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">manual_pipeline_train_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch</span></a><span class="p">):</span>

    <span class="k">def</span> <span class="nf">loss_func</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch</span></a><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">])</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">out</span> <span class="o">-</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch</span></a><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="c1"># We use `alpa.grad` here to seperate the apply gradient stage with the</span>
    <span class="c1"># forward/backward stages in the pipeline. This is necessary to ensure that</span>
    <span class="c1"># the gradient accumulation is correct.</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">alpa</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_func</span><span class="p">)(</span><span class="n">state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">new_state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads</span><span class="o">=</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_state</span>


<span class="n">manual_pipeline_actual_state</span> <span class="o">=</span> <span class="n">manual_pipeline_train_step</span><span class="p">(</span>
    <span class="n">manual_pipeline_state</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch</span></a><span class="p">)</span>
<span class="n">assert_allclose</span><span class="p">(</span><span class="n">expected_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span>
                <span class="n">manual_pipeline_actual_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span>
                <span class="n">atol</span><span class="o">=</span><span class="mf">5e-3</span><span class="p">)</span>

<span class="n">alpa</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In addition, Alpa supports more flexible manual assignments of pipeline
parallelism strategies. In the above example, each partitioned stages will
be assigned an equal number of devices to run. If you want to control the
device assignment of each stage, you can use the more advanced
<code class="docutils literal notranslate"><span class="pre">stage_option=alpa.ManualStageOption</span></code>.</p>
</div>
</section>
<section id="pipeline-parallelism-with-automatic-assignment">
<h2>Pipeline Parallelism with Automatic Assignment<a class="headerlink" href="#pipeline-parallelism-with-automatic-assignment" title="Permalink to this headline"></a></h2>
<p>Alpa also supports automatically partitioning the model into multiple
pipeline stages and assign each pipeline stage a device mesh such that
the total execution latency is minimized. Specifically, the automatic
partitioning algorithm consists of the following steps:</p>
<ol class="arabic simple">
<li><p><strong>Layer Construction:</strong> In this step, the operators in the model are
clustered into “layers” based on a graph clustering algorithm. The
user needs to specify the total number of layers (i.e. clusters) as
a hyperparameter.</p></li>
<li><p><strong>Stage Construction and Mesh Slicing:</strong> In this step, we partition
the device cluster (device mesh) to multiple submeshes and assign
layers to submeshes to form pipeline stages to minimize the total
pipeline execution latency.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">alpa</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">cluster</span><span class="o">=</span><span class="s2">&quot;ray&quot;</span><span class="p">)</span>

<span class="c1"># Define the parallel method.</span>
<span class="c1"># `alpa.AutoLayerOption(layer_num=2)` means we use the auto layer construcion</span>
<span class="c1"># algorithm to cluster primitive operators into two layers.</span>
<span class="c1"># `stage_option=&quot;auto&quot;` means we enable the auto stage construction algorithm.</span>
<span class="n">method</span> <span class="o">=</span> <a href="https://docs.python.org/3/library/abc.html#abc.ABC" title="abc.ABC" class="sphx-glr-backref-module-abc sphx-glr-backref-type-py-class"><span class="n">alpa</span><span class="o">.</span><span class="n">PipeshardParallel</span></a><span class="p">(</span><span class="n">num_micro_batches</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                                <span class="n">layer_option</span><span class="o">=</span><a href="https://docs.python.org/3/library/abc.html#abc.ABC" title="abc.ABC" class="sphx-glr-backref-module-abc sphx-glr-backref-type-py-class"><span class="n">alpa</span><span class="o">.</span><span class="n">AutoLayerOption</span></a><span class="p">(</span><span class="n">layer_num</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
                                <span class="n">stage_option</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>


<span class="c1"># Define the training step. The function body is the same as the above one.</span>
<span class="nd">@alpa</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">auto_pipeline_train_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch</span></a><span class="p">):</span>

    <span class="k">def</span> <span class="nf">loss_func</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch</span></a><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">])</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">out</span> <span class="o">-</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch</span></a><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="c1"># Again, we use `alpa.grad` here to seperate the apply gradient stage with</span>
    <span class="c1"># the forward/backward stages in the pipeline.</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">alpa</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_func</span><span class="p">)(</span><span class="n">state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">new_state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads</span><span class="o">=</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_state</span>


<span class="c1"># In the first call, alpa triggers the compilation.</span>
<span class="c1"># The compilation first profiles several costs and solves an optimization</span>
<span class="c1"># problem to get the optimal pipeline assignments.</span>
<span class="n">auto_pipeline_actual_state</span> <span class="o">=</span> <span class="n">auto_pipeline_train_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">batch</span></a><span class="p">)</span>
<span class="n">assert_allclose</span><span class="p">(</span><span class="n">expected_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span>
                <span class="n">auto_pipeline_actual_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span>
                <span class="n">atol</span><span class="o">=</span><span class="mf">5e-3</span><span class="p">)</span>

<span class="n">alpa</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>WARNING:alpa.pipeline_parallel.layer_construction:Too few non-trivial ops (dot, conv), which may influence auto-sharding performance
WARNING:alpa.pipeline_parallel.layer_construction:Too few non-trivial ops (dot, conv), which may influence auto-sharding performance
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (1, 4), (1, 8))
- Profiling for submesh 3 (1, 8):
- Generate all stage infos (Jaxpr -&gt; HLO)

  0%|          | 0/2 [00:00&lt;?, ?it/s]

  0%|          | 0/2 [00:00&lt;?, ?it/s]



  0%|          | 0/1 [00:00&lt;?, ?it/s]


100%|##########| 2/2 [00:00&lt;00:00, 31.98it/s]
- Compile all stages

  0%|          | 0/5 [00:00&lt;?, ?it/s]
 20%|##        | 1/5 [00:01&lt;00:07,  1.79s/it]
 60%|######    | 3/5 [00:01&lt;00:01,  1.98it/s]
100%|##########| 5/5 [00:02&lt;00:00,  2.50it/s]
100%|##########| 5/5 [00:02&lt;00:00,  1.99it/s]
- Profile all stages

  0%|          | 0/5 [00:00&lt;?, ?it/s]

cost[0, 1, 0]=0.004, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=1.001GB, intermediate=0.000GB, init=0.500GB, as_config=((8, 1), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

  0%|          | 0/5 [00:22&lt;?, ?it/s]
 20%|##        | 1/5 [00:22&lt;01:31, 22.96s/it]

cost[0, 1, 1]=0.003, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=0.501GB, intermediate=0.000GB, init=0.250GB, as_config=((4, 2), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 20%|##        | 1/5 [00:25&lt;01:31, 22.96s/it]
 40%|####      | 2/5 [00:25&lt;00:32, 10.86s/it]

cost[0, 1, 2]=0.002, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=0.251GB, intermediate=0.000GB, init=0.125GB, as_config=((2, 4), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 40%|####      | 2/5 [00:28&lt;00:32, 10.86s/it]
 60%|######    | 3/5 [00:28&lt;00:14,  7.15s/it]

cost[0, 1, 3]=0.002, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=0.098GB, intermediate=0.000GB, init=0.063GB, as_config=((1, 8), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 60%|######    | 3/5 [00:28&lt;00:14,  7.15s/it]
 80%|########  | 4/5 [00:28&lt;00:04,  4.52s/it]

cost[0, 1, 4]=0.002, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=0.098GB, intermediate=0.000GB, init=0.063GB, as_config=((8, 1), {})

 80%|########  | 4/5 [00:28&lt;00:04,  4.52s/it]
100%|##########| 5/5 [00:28&lt;00:00,  2.96s/it]
100%|##########| 5/5 [00:28&lt;00:00,  5.75s/it]
Profiling for submesh 3 (1, 8) takes 31.67 seconds
Profiled costs are: [[[       inf        inf        inf        inf        inf]
  [0.0044566  0.00284976 0.00207575 0.00167309 0.00159558]]

 [[       inf        inf        inf        inf        inf]
  [       inf        inf        inf        inf        inf]]]
Profiled max_n_succ_stages are: [[[  -1   -1   -1   -1   -1]
  [4096 4096 4096 4096 4096]]

 [[  -1   -1   -1   -1   -1]
  [  -1   -1   -1   -1   -1]]]
--------------------------------------------------
- Profiling for submesh 2 (1, 4):
- Generate all stage infos (Jaxpr -&gt; HLO)

  0%|          | 0/2 [00:00&lt;?, ?it/s]

  0%|          | 0/2 [00:00&lt;?, ?it/s]



  0%|          | 0/1 [00:00&lt;?, ?it/s]


100%|##########| 2/2 [00:00&lt;00:00, 15.61it/s]
100%|##########| 2/2 [00:00&lt;00:00, 15.58it/s]
- Compile all stages

  0%|          | 0/12 [00:00&lt;?, ?it/s]
  8%|8         | 1/12 [00:01&lt;00:18,  1.72s/it]
 50%|#####     | 6/12 [00:01&lt;00:01,  4.36it/s]
 83%|########3 | 10/12 [00:02&lt;00:00,  6.90it/s]
100%|##########| 12/12 [00:02&lt;00:00,  4.61it/s]
- Profile all stages

  0%|          | 0/12 [00:00&lt;?, ?it/s]

cost[0, 0, 1]=0.002, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=0.252GB, intermediate=0.001GB, init=0.125GB, as_config=((2, 2), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

  0%|          | 0/12 [00:12&lt;?, ?it/s]
  8%|8         | 1/12 [00:12&lt;02:13, 12.13s/it]

cost[0, 0, 2]=0.002, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=0.099GB, intermediate=0.001GB, init=0.063GB, as_config=((1, 4), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

  8%|8         | 1/12 [00:13&lt;02:13, 12.13s/it]
 17%|#6        | 2/12 [00:13&lt;00:58,  5.87s/it]

cost[0, 0, 3]=0.001, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=0.099GB, intermediate=0.001GB, init=0.063GB, as_config=((4, 1), {})

 17%|#6        | 2/12 [00:13&lt;00:58,  5.87s/it]
 25%|##5       | 3/12 [00:13&lt;00:29,  3.24s/it]

cost[0, 0, 0]=0.002, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=0.501GB, intermediate=0.001GB, init=0.250GB, as_config=((4, 1), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 25%|##5       | 3/12 [00:14&lt;00:29,  3.24s/it]
 33%|###3      | 4/12 [00:14&lt;00:16,  2.07s/it]

cost[0, 1, 1]=0.004, max_n_succ_stage=3525, Mem: avail=13.664GB, peak=0.501GB, intermediate=0.004GB, init=0.250GB, as_config=((2, 2), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 33%|###3      | 4/12 [00:15&lt;00:16,  2.07s/it]
 42%|####1     | 5/12 [00:15&lt;00:14,  2.01s/it]

cost[0, 1, 0]=0.004, max_n_succ_stage=3984, Mem: avail=13.664GB, peak=1.001GB, intermediate=0.003GB, init=0.500GB, as_config=((4, 1), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 42%|####1     | 5/12 [00:16&lt;00:14,  2.01s/it]
 50%|#####     | 6/12 [00:16&lt;00:08,  1.37s/it]

cost[0, 1, 3]=0.002, max_n_succ_stage=2732, Mem: avail=13.664GB, peak=0.194GB, intermediate=0.005GB, init=0.125GB, as_config=((4, 1), {})

 50%|#####     | 6/12 [00:16&lt;00:08,  1.37s/it]
 58%|#####8    | 7/12 [00:16&lt;00:05,  1.02s/it]

cost[0, 1, 2]=0.002, max_n_succ_stage=2732, Mem: avail=13.664GB, peak=0.194GB, intermediate=0.005GB, init=0.125GB, as_config=((1, 4), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 58%|#####8    | 7/12 [00:16&lt;00:05,  1.02s/it]

cost[1, 1, 0]=0.003, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=0.501GB, intermediate=0.002GB, init=0.250GB, as_config=((4, 1), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 58%|#####8    | 7/12 [00:16&lt;00:05,  1.02s/it]
 75%|#######5  | 9/12 [00:16&lt;00:01,  1.69it/s]

cost[1, 1, 1]=0.002, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=0.252GB, intermediate=0.002GB, init=0.125GB, as_config=((2, 2), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 75%|#######5  | 9/12 [00:16&lt;00:01,  1.69it/s]

cost[1, 1, 2]=0.001, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=0.099GB, intermediate=0.002GB, init=0.063GB, as_config=((1, 4), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 75%|#######5  | 9/12 [00:16&lt;00:01,  1.69it/s]
 92%|#########1| 11/12 [00:16&lt;00:00,  2.58it/s]

cost[1, 1, 3]=0.001, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=0.099GB, intermediate=0.002GB, init=0.063GB, as_config=((4, 1), {})

 92%|#########1| 11/12 [00:16&lt;00:00,  2.58it/s]
100%|##########| 12/12 [00:16&lt;00:00,  1.40s/it]
Profiling for submesh 2 (1, 4) takes 19.97 seconds
Profiled costs are: [[[0.00230087 0.00237221 0.00150524 0.0012798         inf]
  [0.00436388 0.00430009 0.00203482 0.00226185        inf]]

 [[       inf        inf        inf        inf        inf]
  [0.00299427 0.00184274 0.00125347 0.00129188        inf]]]
Profiled max_n_succ_stages are: [[[4096 4096 4096 4096   -1]
  [3984 3525 2732 2732   -1]]

 [[  -1   -1   -1   -1   -1]
  [4096 4096 4096 4096   -1]]]
--------------------------------------------------
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -&gt; HLO)

  0%|          | 0/2 [00:00&lt;?, ?it/s]

  0%|          | 0/2 [00:00&lt;?, ?it/s]



  0%|          | 0/1 [00:00&lt;?, ?it/s]


100%|##########| 2/2 [00:00&lt;00:00, 13.85it/s]
100%|##########| 2/2 [00:00&lt;00:00, 13.82it/s]
- Compile all stages

  0%|          | 0/9 [00:00&lt;?, ?it/s]
 11%|#1        | 1/9 [00:01&lt;00:13,  1.63s/it]
 89%|########8 | 8/9 [00:01&lt;00:00,  6.02it/s]
100%|##########| 9/9 [00:01&lt;00:00,  5.06it/s]
- Profile all stages

  0%|          | 0/9 [00:00&lt;?, ?it/s]

cost[0, 0, 2]=0.002, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=0.194GB, intermediate=0.003GB, init=0.125GB, as_config=((2, 1), {})

  0%|          | 0/9 [00:10&lt;?, ?it/s]
 11%|#1        | 1/9 [00:10&lt;01:22, 10.34s/it]

cost[0, 0, 1]=0.002, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=0.194GB, intermediate=0.003GB, init=0.125GB, as_config=((1, 2), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 11%|#1        | 1/9 [00:10&lt;01:22, 10.34s/it]

cost[0, 1, 1]=0.003, max_n_succ_stage=1778, Mem: avail=13.664GB, peak=0.383GB, intermediate=0.007GB, init=0.250GB, as_config=((1, 2), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 11%|#1        | 1/9 [00:10&lt;01:22, 10.34s/it]
 33%|###3      | 3/9 [00:10&lt;00:17,  2.86s/it]

cost[0, 1, 2]=0.003, max_n_succ_stage=1778, Mem: avail=13.664GB, peak=0.383GB, intermediate=0.007GB, init=0.250GB, as_config=((2, 1), {})

 33%|###3      | 3/9 [00:10&lt;00:17,  2.86s/it]

cost[1, 1, 1]=0.002, max_n_succ_stage=3643, Mem: avail=13.664GB, peak=0.194GB, intermediate=0.004GB, init=0.125GB, as_config=((1, 2), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 33%|###3      | 3/9 [00:11&lt;00:17,  2.86s/it]
 56%|#####5    | 5/9 [00:11&lt;00:05,  1.46s/it]

cost[0, 0, 0]=0.003, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=0.502GB, intermediate=0.003GB, init=0.250GB, as_config=((2, 1), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 56%|#####5    | 5/9 [00:11&lt;00:05,  1.46s/it]

cost[1, 1, 2]=0.002, max_n_succ_stage=3643, Mem: avail=13.664GB, peak=0.194GB, intermediate=0.004GB, init=0.125GB, as_config=((2, 1), {})

 56%|#####5    | 5/9 [00:11&lt;00:05,  1.46s/it]
 78%|#######7  | 7/9 [00:11&lt;00:01,  1.14it/s]

cost[0, 1, 0]=0.005, max_n_succ_stage=1991, Mem: avail=13.664GB, peak=1.001GB, intermediate=0.006GB, init=0.500GB, as_config=((2, 1), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 78%|#######7  | 7/9 [00:11&lt;00:01,  1.14it/s]
 89%|########8 | 8/9 [00:11&lt;00:00,  1.34it/s]

cost[1, 1, 0]=0.003, max_n_succ_stage=4096, Mem: avail=13.664GB, peak=0.502GB, intermediate=0.003GB, init=0.250GB, as_config=((2, 1), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 89%|########8 | 8/9 [00:12&lt;00:00,  1.34it/s]
100%|##########| 9/9 [00:12&lt;00:00,  1.34it/s]
100%|##########| 9/9 [00:12&lt;00:00,  1.37s/it]
Profiling for submesh 1 (1, 2) takes 14.58 seconds
Profiled costs are: [[[0.00261346 0.00154781 0.00157099        inf        inf]
  [0.00514685 0.00304916 0.00301576        inf        inf]]

 [[       inf        inf        inf        inf        inf]
  [0.00281758 0.00181362 0.00182936        inf        inf]]]
Profiled max_n_succ_stages are: [[[4096 4096 4096   -1   -1]
  [1991 1778 1778   -1   -1]]

 [[  -1   -1   -1   -1   -1]
  [4096 3643 3643   -1   -1]]]
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -&gt; HLO)

  0%|          | 0/2 [00:00&lt;?, ?it/s]

  0%|          | 0/2 [00:00&lt;?, ?it/s]

100%|##########| 2/2 [00:00&lt;00:00, 18.98it/s]


 50%|#####     | 1/2 [00:00&lt;00:00,  9.39it/s]

  0%|          | 0/1 [00:00&lt;?, ?it/s]


100%|##########| 2/2 [00:00&lt;00:00, 14.38it/s]
- Compile all stages

  0%|          | 0/6 [00:00&lt;?, ?it/s]
 17%|#6        | 1/6 [00:01&lt;00:07,  1.50s/it]
100%|##########| 6/6 [00:01&lt;00:00,  4.94it/s]
100%|##########| 6/6 [00:01&lt;00:00,  3.74it/s]
- Profile all stages

  0%|          | 0/6 [00:00&lt;?, ?it/s]

cost[0, 1, 0]=0.005, max_n_succ_stage=1014, Mem: avail=13.664GB, peak=0.763GB, intermediate=0.012GB, init=0.500GB, as_config=((1, 1), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

  0%|          | 0/6 [00:10&lt;?, ?it/s]
 17%|#6        | 1/6 [00:10&lt;00:50, 10.19s/it]

cost[1, 1, 0]=0.003, max_n_succ_stage=2134, Mem: avail=13.664GB, peak=0.383GB, intermediate=0.006GB, init=0.250GB, as_config=((1, 1), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 17%|#6        | 1/6 [00:10&lt;00:50, 10.19s/it]
 33%|###3      | 2/6 [00:10&lt;00:17,  4.29s/it]

cost[0, 0, 1]=0.002, max_n_succ_stage=2540, Mem: avail=13.664GB, peak=0.384GB, intermediate=0.005GB, init=0.250GB, as_config=((1, 1), {})

 33%|###3      | 2/6 [00:10&lt;00:17,  4.29s/it]

cost[1, 1, 1]=0.003, max_n_succ_stage=2134, Mem: avail=13.664GB, peak=0.383GB, intermediate=0.006GB, init=0.250GB, as_config=((1, 1), {})

 33%|###3      | 2/6 [00:10&lt;00:17,  4.29s/it]

cost[0, 0, 0]=0.002, max_n_succ_stage=2540, Mem: avail=13.664GB, peak=0.384GB, intermediate=0.005GB, init=0.250GB, as_config=((1, 1), {&#39;force_batch_dim_to_mesh_dim&#39;: 0})

 33%|###3      | 2/6 [00:10&lt;00:17,  4.29s/it]
 83%|########3 | 5/6 [00:10&lt;00:01,  1.25s/it]

cost[0, 1, 1]=0.005, max_n_succ_stage=1014, Mem: avail=13.664GB, peak=0.763GB, intermediate=0.012GB, init=0.500GB, as_config=((1, 1), {})

 83%|########3 | 5/6 [00:10&lt;00:01,  1.25s/it]
100%|##########| 6/6 [00:10&lt;00:00,  1.76s/it]
Profiling for submesh 0 (1, 1) takes 12.62 seconds
Profiled costs are: [[[0.00240209 0.00232538        inf        inf        inf]
  [0.00472821 0.0047871         inf        inf        inf]]

 [[       inf        inf        inf        inf        inf]
  [0.00267063 0.00267424        inf        inf        inf]]]
Profiled max_n_succ_stages are: [[[2540 2540   -1   -1   -1]
  [1014 1014   -1   -1   -1]]

 [[  -1   -1   -1   -1   -1]
  [2134 2134   -1   -1   -1]]]
--------------------------------------------------
Compute cost saved to: compute-cost-2022-07-01-23-13-36.npy
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0], [1]]
Result mesh_shapes: [(1, 4), (1, 4)]
Result logical_mesh_shapes: [(4, 1), (1, 4)]
Result autosharding_option_dicts: [{}, {&#39;force_batch_dim_to_mesh_dim&#39;: 0}]
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 2 minutes  29.092 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-tutorials-pipeshard-parallelism-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/a3ca1faed62a74bef9de2bc3edfc7d0a/pipeshard_parallelism.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">pipeshard_parallelism.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/119275878b2983d17fa8261c96cf7ace/pipeshard_parallelism.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">pipeshard_parallelism.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="quickstart.html" class="btn btn-neutral float-left" title="Alpa Quickstart" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="alpa_vs_pmap.html" class="btn btn-neutral float-right" title="Differences between alpa.parallelize and jax.pmap" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>