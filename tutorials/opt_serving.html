<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Serving OPT-175B, BLOOM-176B and CodeGen-16B using Alpa &mdash; Alpa 0.2.3.dev17 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/alpa-logo.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Performance Tuning Guide" href="perf_tuning_guide.html" />
    <link rel="prev" title="Differences between alpa.parallelize, jax.pmap and jax.pjit" href="alpa_vs_pmap.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Alpa
          </a>
              <div class="version">
                0.2.3.dev17
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install Alpa</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Alpa Quickstart</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="pipeshard_parallelism.html">Distributed Training with Both Shard and Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="alpa_vs_pmap.html">Differences between alpa.parallelize, jax.pmap and jax.pjit</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Serving OPT-175B, BLOOM-176B and CodeGen-16B using Alpa</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#demo">Demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="#convert-weights-format">Convert Weights Format</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#convert-opt-175b-weights-into-alpa-formats">Convert OPT-175B weights into Alpa formats</a></li>
<li class="toctree-l3"><a class="reference internal" href="#converted-weights-for-other-models">Converted weights for other models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#copy-weights-to-multiple-nodes">Copy Weights to Multiple Nodes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#run-generation-in-the-command-line">Run Generation in the Command Line</a></li>
<li class="toctree-l2"><a class="reference internal" href="#launch-a-web-server-to-serve-the-opt-models">Launch a Web Server to Serve the OPT Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#improving-generation-speed">Improving Generation Speed</a></li>
<li class="toctree-l2"><a class="reference internal" href="#opt-license">OPT License</a></li>
<li class="toctree-l2"><a class="reference internal" href="#other-models-bloom">Other Models (BLOOM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#other-models-codegen">Other Models (CodeGen)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="perf_tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="icml_big_model_tutorial.html">ICML‚Äô22 Big Model Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="alpa_on_slurm.html">Using Alpa on Slurm</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions (FAQ)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Architecture</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture/overview.html">Design and Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/alpa_compiler_walk_through.html">Alpa Compiler Walk-Through</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/intra_op_solver.html">Code Structure of the Intra-op Solver</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Benchmark</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/benchmark.html">Performance Benchmark</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Publications</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../publications/publications.html">Publications</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../developer/developer_guide.html">Developer Guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Alpa</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Serving OPT-175B, BLOOM-176B and CodeGen-16B using Alpa</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/alpa-projects/alpa/blob/main/docs/tutorials/opt_serving.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="serving-opt-175b-bloom-176b-and-codegen-16b-using-alpa">
<h1>Serving OPT-175B, BLOOM-176B and CodeGen-16B using Alpa<a class="headerlink" href="#serving-opt-175b-bloom-176b-and-codegen-16b-using-alpa" title="Permalink to this headline">ÔÉÅ</a></h1>
<p>This tutorial shows how to setup a serving system to serve one of the largest available pretrained language models <a class="reference external" href="https://github.com/facebookresearch/metaseq/tree/main/projects/OPT">OPT-175B</a>. The instructions for other models (BLOOM and CodeGen) are also listed at the end.</p>
<p>üëâ Try a live demo at <a class="reference external" href="https://alpa-projects.github.io/opt">Alpa-OPT Demo</a> üëà</p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>As a serving system, Alpa offers the following unique advantages:</p>
<ul class="simple">
<li><p><strong>Designed for large models</strong>: Cannot fit the model into a single GPU? Not a problem, Alpa is designed for training and serving big models like GPT-3.</p></li>
<li><p><strong>Support commodity hardware</strong>: With Alpa, you can serve OPT-175B using your in-house GPU cluster, without needing the latest generations of A100 80GB GPUs nor fancy InfiniBand connections ‚Äì no hardware constraints!</p></li>
<li><p><strong>Flexible parallelism strategies</strong>: Alpa will automatically figure out the appropriate model-parallel strategies based on your cluster setup and your model architecture.</p></li>
</ul>
<p>In this example, we use Alpa to serve the open-source OPT model, supporting all sizes ranging from 125M to 175B. Specifically, Alpa provides:</p>
<ul class="simple">
<li><p>A distributed backend to perform efficient model-parallel inference for the large OPT models.</p></li>
<li><p>A web frontend to collect and batch inference requests from users.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The pre-trained OPT model weights can be obtained from <a class="reference external" href="https://github.com/facebookresearch/metaseq">Metaseq</a>, subject to their license.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You will need at least 350GB GPU memory on your entire cluster to serve the OPT-175B model.
For example, you can use 4 x AWS p3.16xlarge instances, which provide 4 (instance) x 8 (GPU/instance) x 16 (GB/GPU) = 512 GB memory.</p>
<p>You can also follow this guide to setup a serving system to serve smaller versions of OPT, such as OPT-66B, OPT-30B, etc.
Pick an appropriate size from <a class="reference external" href="https://github.com/facebookresearch/metaseq/tree/main/projects/OPT">OPT weight downloading page</a> based on your available resources.</p>
</div>
</section>
<section id="demo">
<h2>Demo<a class="headerlink" href="#demo" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>The code below shows how to use huggingface/transformers interface and Alpa distributed backend for large model inference.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span> <span class="nn">llm_serving.model.wrapper</span> <span class="kn">import</span> <span class="n">get_model</span>

<span class="c1"># Load the tokenizer. All OPT models with different sizes share the same tokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/opt-2.7b&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">add_bos_token</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Load the model. Alpa automatically downloads the weights to the specificed path</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;alpa/opt-2.7b&quot;</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="s2">&quot;~/opt_weights/&quot;</span><span class="p">)</span>

<span class="c1"># Generate</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Paris is the capital city of&quot;</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input_ids</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">generated_string</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">generated_string</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="requirements">
<h2>Requirements<a class="headerlink" href="#requirements" title="Permalink to this headline">ÔÉÅ</a></h2>
<ol class="arabic simple">
<li><p>Install Alpa following the <a class="reference external" href="https://alpa-projects.github.io/install.html">installation guide</a>. You can either install by python wheel or build from source.</p></li>
<li><p>Install additional requirements for <code class="docutils literal notranslate"><span class="pre">llm_serving</span></code>:</p></li>
</ol>
<blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip3 install <span class="s2">&quot;transformers&lt;=4.23.1&quot;</span> fastapi uvicorn omegaconf jinja2

<span class="c1"># Install torch corresponding to your CUDA version, e.g., for CUDA 11.3:</span>
pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113
</pre></div>
</div>
</div></blockquote>
<ol class="arabic simple" start="3">
<li><p>Clone the <code class="docutils literal notranslate"><span class="pre">alpa</span></code> repo. If you install alpa by python wheel, please clone the alpa repo. If you install from source, you already did this step.</p></li>
</ol>
<blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git clone git@github.com:alpa-projects/alpa.git
</pre></div>
</div>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p>Install <code class="docutils literal notranslate"><span class="pre">llm_serving</span></code> package. Go to the examples folder and install the package.</p></li>
</ol>
<blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> alpa/examples
pip3 install -e .
</pre></div>
</div>
</div></blockquote>
</section>
<section id="convert-weights-format">
<h2>Convert Weights Format<a class="headerlink" href="#convert-weights-format" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>The weights of OPT 125M‚Äì66B models are publicly available. Huggingface hosts copies of these weights.
For OPT 125M‚Äì66B, you <strong>do not need</strong> to download or convert the weights manually. Alpa will automatically download the weights from huggingface to the given path if Alpa cannot find cached weights locally.</p>
<p>The weights of OPT-175B can be got from meta by filling a <a class="reference external" href="https://github.com/facebookresearch/metaseq/tree/main/projects/OPT">request form</a> .
You then need to manually convert the obtained weights into Alpa format.</p>
<section id="convert-opt-175b-weights-into-alpa-formats">
<h3>Convert OPT-175B weights into Alpa formats<a class="headerlink" href="#convert-opt-175b-weights-into-alpa-formats" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>We provide detailed instructions below on how to convert the original OPT-175B weights into Alpa-compatible formats. You can skip this section if you only want to run smaller models.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The procedures below for converting OPT-175B weights will take about 1 hour.</p>
</div>
</div></blockquote>
<ol class="arabic simple">
<li><dl class="simple">
<dt>Download and verify the original weights</dt><dd><p>First, download Metaseq‚Äôs original OPT-175B weights in 992 shards, verify the <a class="reference external" href="https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/assets/opt175b_md5sum_shards.csv">MD5 of each shard</a> , and put the shards under a folder, say, <code class="docutils literal notranslate"><span class="pre">PATH_TO_992_SHARDS/</span></code>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Consolidate the weights from 992 shards into one single checkpoint</dt><dd><p>Use the script <a class="reference external" href="https://github.com/alpa-projects/alpa/tree/main/examples/llm_serving/scripts/step_2_consolidate_992_shards_to_singleton.py">step_2_consolidate_992_shards_to_singleton.py</a> as:</p>
</dd>
</dl>
</li>
</ol>
<blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3 step_2_consolidate_992_shards_to_singleton.py --read-prefix <span class="o">[</span>PATH_TO_992_SHARDS<span class="o">]</span>/checkpoint_last --save-prefix <span class="o">[</span>PATH_TO_SAVE_CHECKPOINT<span class="o">]</span>
</pre></div>
</div>
<p>The consolidated checkpoint will be saved at <code class="docutils literal notranslate"><span class="pre">PATH_TO_SAVE_CHECKPOINT</span></code> as specified in the command.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The above script will require a peak memory (RAM) usage as large as twice of the model size.
For example, if you are performing consolidation for the 175B model, it will approximately have a peak memory usage of 175B x 2 bytes x 2 = 700GB.
Please make sure your RAM is sufficient to run the script without throwing an OOM exception.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The above script will save the model weights as a single consolidated checkpoint at <code class="docutils literal notranslate"><span class="pre">PATH_TO_SAVE_CHECKPOINT</span></code>, hence will require at least 350GB disk space available.</p>
</div>
</div></blockquote>
<ol class="arabic" start="3">
<li><dl>
<dt>Convert the single checkpoint into Alpa-compatible formats</dt><dd><p>Alpa ingests weights simply from numpy formats. Use the script <a class="reference external" href="https://github.com/alpa-projects/alpa/tree/main/examples/llm_serving/scripts/step_3_convert_to_numpy_weights.py">step_3_convert_to_numpy_weights.py</a> to convert the
single checkpoint into numpy formats:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3 step_3_convert_to_numpy_weights.py --ckpt-path PATH_TO_SAVE_CHECKPOINT --output-folder OUTPUT_PATH
</pre></div>
</div>
<p>The weights will be saved at the folder <code class="docutils literal notranslate"><span class="pre">OUTPUT_PATH</span></code> as specified in the command.</p>
</dd>
</dl>
</li>
</ol>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The above script also requires 350GB free disk space to write the numpy-formatted weights.</p>
</div>
</div></blockquote>
</section>
<section id="converted-weights-for-other-models">
<h3>Converted weights for other models<a class="headerlink" href="#converted-weights-for-other-models" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>You do not need to download the weights manually for OPT 125M‚Äì66B. However, if you have trouble with the automatic downloading or huggingface. We also provide the converted weights for the following models.</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://drive.google.com/file/d/1Ps7DFD80wNO7u2t39YCYcBX-9XwypGzl/view?usp=sharing">OPT-125M weights</a></p></li>
<li><p><a class="reference external" href="https://drive.google.com/file/d/1ayIaKRhxF9osZWgcFG-3vSkjcepSWdQd/view?usp=sharing">OPT-2.7B weights</a></p></li>
<li><p><a class="reference external" href="https://drive.google.com/file/d/1_MBcgwTqHFboV0JkGWR03AOHusrxcHlu/view?usp=sharing">OPT-30B weights</a></p></li>
</ul>
</div></blockquote>
</section>
<section id="copy-weights-to-multiple-nodes">
<h3>Copy Weights to Multiple Nodes<a class="headerlink" href="#copy-weights-to-multiple-nodes" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>If you want to run the model on multiple nodes, you can use one of the following methods to copy the weights to all nodes.</p>
<ol class="arabic simple">
<li><p>Put the weights under a shared network file system, so all nodes can access it.</p></li>
<li><p>Run the script first on a driver node. The driver node will download the weights to its local disk, but the script will fail later because worker nodes cannot access the weights.
You can then manually copy all downloaded weights under <code class="docutils literal notranslate"><span class="pre">path</span></code> from the driver node to all worker nodes.</p></li>
</ol>
</section>
</section>
<section id="run-generation-in-the-command-line">
<h2>Run Generation in the Command Line<a class="headerlink" href="#run-generation-in-the-command-line" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>The code of this tutorial is under <a class="reference external" href="https://github.com/alpa-projects/alpa/tree/main/examples/llm_serving">examples/llm_serving</a>.</p>
<ul>
<li><p>Run generation using the 125M model with PyTorch/HuggingFace backend on a single GPU:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3 textgen.py --model facebook/opt-125m
</pre></div>
</div>
</li>
<li><p>Run generation using the 125M model with JAX backend on a single GPU:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3 textgen.py --model jax/opt-125m
</pre></div>
</div>
</li>
<li><p>Run model-parallel generation using the 2.7B model with Alpa on multiple GPUs:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start ray on the node</span>
ray start --head

python3 textgen.py --model alpa/opt-2.7b
</pre></div>
</div>
</li>
<li><p>Run distributed generation using the 175B model with Alpa on a cluster of GPU nodes.
Note you will need &gt;350GB total GPU memory in the entire cluster to successfully run the inference.</p>
<p>Before running the command below, start Ray on the cluster following <a class="reference external" href="https://docs.ray.io/en/latest/cluster/cloud.html#manual-cluster">this guide</a>. You can check the cluster status by <code class="docutils literal notranslate"><span class="pre">ray</span> <span class="pre">status</span></code>. You should be able to see all GPUs and all nodes in the output.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3 textgen.py --model alpa/opt-175b
</pre></div>
</div>
</li>
</ul>
</section>
<section id="launch-a-web-server-to-serve-the-opt-models">
<h2>Launch a Web Server to Serve the OPT Models<a class="headerlink" href="#launch-a-web-server-to-serve-the-opt-models" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>We need to run two scripts: one for web server and another for the model serving worker.
They will use two ports. The port of the website is defined in the command line and the port of the worker is defined in <code class="docutils literal notranslate"><span class="pre">service/constants.py</span></code></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Launch the model worker</span>
python3 launch_model_worker.py --model alpa/opt-175b

<span class="c1"># Launch the website (in a new terminal)</span>
uvicorn launch_website:app --host <span class="m">0</span>.0.0.0 --port <span class="m">8001</span>
</pre></div>
</div>
<p>Then open <code class="docutils literal notranslate"><span class="pre">http://[IP-ADDRESS]:8001</span></code> in your browser to try out the model!</p>
<p>There is also a client library which can be used to query the model worker
via a python script. Please check <code class="docutils literal notranslate"><span class="pre">test_completions.py</span></code> for the usage.</p>
</section>
<section id="improving-generation-speed">
<h2>Improving Generation Speed<a class="headerlink" href="#improving-generation-speed" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>Here are some tips for improving the generation speed.</p>
<ol class="arabic simple">
<li><p>Batching. Single sequence generation cannot fully utilize the GPU power.
Applying batching can greatly boost the performace. See <code class="docutils literal notranslate"><span class="pre">textgen.py</span></code> for the usage.</p></li>
<li><p>Tune the <code class="docutils literal notranslate"><span class="pre">encoder_chunk_sizes</span></code> argument of <code class="docutils literal notranslate"><span class="pre">get_model</span></code>.
Alpa compiles multiple executables and uses these executables to encode a prompt chunk by chunk. This argument controls the possible chunk sizes. Depending on the length of your prompt, you can try different combinations. For example, if your prompt lengths are around 1000-1500, a good combination is <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">256,</span> <span class="pre">1024]</span></code>.</p></li>
<li><p>Tune parallelization strategy. If you are familiar with alpa, you can tune the <code class="docutils literal notranslate"><span class="pre">method</span></code> argument of <code class="docutils literal notranslate"><span class="pre">alpa.parallelize</span></code> and try different parallelization methods.</p></li>
</ol>
<p>If you find the generation speed too slow and want to accelerate it, please join <a class="reference external" href="https://forms.gle/YEZTCrtZD6EAVNBQ7">Alpa slack</a> and tell us your use cases. We are actively working on improving the performance.</p>
</section>
<section id="opt-license">
<h2>OPT License<a class="headerlink" href="#opt-license" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>The use of the OPT pretrained weights is subject to the <a class="reference external" href="https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.md">Model License</a> by Metaseq.</p>
</section>
<section id="other-models-bloom">
<h2>Other Models (BLOOM)<a class="headerlink" href="#other-models-bloom" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>Alpa also supports <a class="reference external" href="https://huggingface.co/bigscience/bloom">BLOOM</a>.
You can use commands similar to OPT but with a different model name.</p>
<blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Huggingface/pytorch backend</span>
python3 textgen.py --model bigscience/bloom-560m

<span class="c1"># Jax backend</span>
python3 textgen.py --model jax/bloom-560m

<span class="c1"># Alpa backend</span>
python3 textgen.py --model alpa/bloom-560m
</pre></div>
</div>
</div></blockquote>
</section>
<section id="other-models-codegen">
<h2>Other Models (CodeGen)<a class="headerlink" href="#other-models-codegen" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>Alpa also supports <a class="reference external" href="https://github.com/salesforce/CodeGen">CodeGen</a>.
You can use commands similar to OPT but with a different model name.</p>
<blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Huggingface/pytorch backend</span>
python3 codegen.py --model Salesforce/codegen-2B-mono

<span class="c1"># Alpa backend</span>
python3 codegen.py --model alpa/codegen-2B-mono
</pre></div>
</div>
</div></blockquote>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="alpa_vs_pmap.html" class="btn btn-neutral float-left" title="Differences between alpa.parallelize, jax.pmap and jax.pjit" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="perf_tuning_guide.html" class="btn btn-neutral float-right" title="Performance Tuning Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Alpa Developers.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-587CCSSRL2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-587CCSSRL2', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>