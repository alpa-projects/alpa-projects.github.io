<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Advanced API Usage &mdash; Alpa 0.0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Difference between alpa.parallelize and jax.pmap" href="alpa_vs_pmap.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Alpa
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Install</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/from_source.html">Install Alpa</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Architecture</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture/overview.html">Design and Architecture</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../developer/developer_guide.html">Developer Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Alpa Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="getting_started.html">Getting Started with Alpa</a></li>
<li class="toctree-l2"><a class="reference internal" href="pipeshard_parallelism.html">Distributed Training with Both Shard and Pipeline Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="alpa_vs_pmap.html">Difference between alpa.parallelize and jax.pmap</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Advanced API Usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#auto-sharding-options">Auto-Sharding Options</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#control-specific-collective-primitive">Control specific collective primitive</a></li>
<li class="toctree-l4"><a class="reference internal" href="#force-to-use-data-parallel">Force to use data parallel</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#specify-inter-operator-parallelism-strategy">Specify inter-operator parallelism strategy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#specify-layer-clustering">Specify layer clustering</a></li>
<li class="toctree-l4"><a class="reference internal" href="#specify-stage-construction">Specify stage construction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#rematerialization-with-layer-construction">Rematerialization with layer construction</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Alpa</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">Alpa Tutorials</a> &raquo;</li>
      <li>Advanced API Usage</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/tutorials/advanced_api_usage.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-tutorials-advanced-api-usage-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="advanced-api-usage">
<span id="sphx-glr-tutorials-advanced-api-usage-py"></span><h1>Advanced API Usage<a class="headerlink" href="#advanced-api-usage" title="Permalink to this headline"></a></h1>
<p>This page will cover some more advanced examples of Alpa.</p>
<p>We first import libraries and create example model and train step functions.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">flax.linen</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">import</span> <span class="nn">optax</span>

<span class="kn">import</span> <span class="nn">alpa</span>
<span class="kn">from</span> <span class="nn">alpa</span> <span class="kn">import</span> <span class="n">global_config</span><span class="p">,</span> <span class="n">parallelize</span>
<span class="kn">from</span> <span class="nn">alpa.device_mesh</span> <span class="kn">import</span> <span class="n">DeviceCluster</span>
<span class="kn">from</span> <span class="nn">alpa.model.bert_model</span> <span class="kn">import</span> <span class="n">BertConfig</span><span class="p">,</span> <span class="n">FlaxBertLayer</span>
<span class="kn">from</span> <span class="nn">alpa.model.model_util</span> <span class="kn">import</span> <span class="n">TrainState</span>
<span class="kn">from</span> <span class="nn">alpa.util</span> <span class="kn">import</span> <span class="n">count_communication_primitives</span><span class="p">,</span> <span class="n">get_ray_namespace_str</span>

<span class="c1"># launch the cluster</span>
<span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">cluster</span> <span class="o">=</span> <span class="n">DeviceCluster</span><span class="p">()</span>
<span class="n">global_config</span><span class="o">.</span><span class="n">devices</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">get_physical_mesh</span><span class="p">()</span>

<span class="c1"># define consts</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">4</span>


<span class="c1"># Define model, train state and train step</span>
<span class="k">class</span> <span class="nc">BertLayerModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">BertConfig</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">FlaxBertLayer</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">create_train_state</span><span class="p">(</span><span class="n">rngkey</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">rngkey</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">tx</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">TrainState</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">apply_fn</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">apply</span><span class="p">,</span>
                              <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
                              <span class="n">tx</span><span class="o">=</span><span class="n">tx</span><span class="p">,</span>
                              <span class="n">dynamic_scale</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">state</span>


<span class="n">rngkey</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">rngkey</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">rngkey</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
<span class="n">attention_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">:</span> <span class="n">attention_mask</span><span class="p">}</span>
<span class="n">bert_config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                         <span class="n">intermediate_size</span><span class="o">=</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span>
                         <span class="n">num_attention_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                         <span class="n">num_hidden_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertLayerModel</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">bert_config</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">create_train_state</span><span class="p">(</span><span class="n">rngkey</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">loss_func</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">])</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">out</span> <span class="o">-</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_func</span><span class="p">)(</span><span class="n">state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">new_state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads</span><span class="o">=</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_state</span>


<span class="c1"># define test utils</span>
<span class="k">def</span> <span class="nf">print_hlo_communication_stats</span><span class="p">(</span><span class="n">hlo_text</span><span class="p">):</span>
    <span class="p">(</span><span class="n">n_total</span><span class="p">,</span> <span class="n">n_all_reduce</span><span class="p">,</span> <span class="n">n_all_gather</span><span class="p">,</span> <span class="n">n_reduce_scatter</span><span class="p">,</span>
     <span class="n">n_all_to_all</span><span class="p">)</span> <span class="o">=</span> <span class="n">count_communication_primitives</span><span class="p">(</span><span class="n">hlo_text</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;#total: </span><span class="si">{</span><span class="n">n_total</span><span class="si">}</span><span class="s2">, #all-reduce: </span><span class="si">{</span><span class="n">n_all_reduce</span><span class="si">}</span><span class="s2">, &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;#all-gather: </span><span class="si">{</span><span class="n">n_all_gather</span><span class="si">}</span><span class="s2">, #reduce-scatter: </span><span class="si">{</span><span class="n">n_reduce_scatter</span><span class="si">}</span><span class="s2">, &quot;</span>
          <span class="sa">f</span><span class="s2">&quot;#all-to-all: </span><span class="si">{</span><span class="n">n_all_to_all</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reset_state</span><span class="p">():</span>
    <span class="k">global</span> <span class="n">state</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">create_train_state</span><span class="p">(</span><span class="n">rngkey</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">])</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-pytb notranslate"><div class="highlight"><pre><span></span><span class="gt">Traceback (most recent call last):</span>
  File <span class="nb">&quot;/home/ubuntu/efs/alpa/docs/gallery/tutorials/advanced_api_usage.py&quot;</span>, line <span class="m">25</span>, in <span class="n">&lt;module&gt;</span>
    <span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
  File <span class="nb">&quot;/home/ubuntu/.local/lib/python3.7/site-packages/ray/_private/client_mode_hook.py&quot;</span>, line <span class="m">105</span>, in <span class="n">wrapper</span>
    <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
  File <span class="nb">&quot;/home/ubuntu/.local/lib/python3.7/site-packages/ray/worker.py&quot;</span>, line <span class="m">956</span>, in <span class="n">init</span>
    <span class="n">job_config</span><span class="o">=</span><span class="n">job_config</span><span class="p">)</span>
  File <span class="nb">&quot;/home/ubuntu/.local/lib/python3.7/site-packages/ray/worker.py&quot;</span>, line <span class="m">1302</span>, in <span class="n">connect</span>
    <span class="n">faulthandler</span><span class="o">.</span><span class="n">enable</span><span class="p">(</span><span class="n">all_threads</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gr">AttributeError</span>: <span class="n">&#39;_LoggingTee&#39; object has no attribute &#39;fileno&#39;</span>
</pre></div>
</div>
<section id="auto-sharding-options">
<h2>Auto-Sharding Options<a class="headerlink" href="#auto-sharding-options" title="Permalink to this headline"></a></h2>
<p>AutoShardingOption is designed to control the inter-operator parallelism more precisely.</p>
<section id="control-specific-collective-primitive">
<h3>Control specific collective primitive<a class="headerlink" href="#control-specific-collective-primitive" title="Permalink to this headline"></a></h3>
<p>Some primitive is not well-supported on specific platforms(e.g. may cause deadlock).
In case of that, they should be excluded in auto-sharding’s optimization space.
We control this by some auto-sharding options.</p>
<p>In some cases, an allreduce can be replaced by a reduce-scatter first,
and an all-gather later. The two has the same communication, but reduce-scatter
may readuce the peak memory.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">as_option</span> <span class="o">=</span> <span class="n">global_config</span><span class="o">.</span><span class="n">default_autosharding_option</span>
<span class="n">as_option_backup</span> <span class="o">=</span> <span class="n">as_option</span><span class="o">.</span><span class="n">backup</span><span class="p">()</span>

<span class="n">as_option</span><span class="o">.</span><span class="n">prefer_reduce_scatter</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">executable</span> <span class="o">=</span> <span class="n">parallelize</span><span class="p">(</span><span class="n">train_step</span><span class="p">)</span><span class="o">.</span><span class="n">get_executable</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
<span class="n">print_hlo_communication_stats</span><span class="p">(</span><span class="n">executable</span><span class="o">.</span><span class="n">get_hlo_text</span><span class="p">())</span>

<span class="c1"># create new state to avoid jit</span>
<span class="n">as_option</span><span class="o">.</span><span class="n">prefer_reduce_scatter</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">create_train_state</span><span class="p">(</span><span class="n">rngkey</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">])</span>
<span class="n">executable</span> <span class="o">=</span> <span class="n">parallelize</span><span class="p">(</span><span class="n">train_step</span><span class="p">)</span><span class="o">.</span><span class="n">get_executable</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
<span class="n">print_hlo_communication_stats</span><span class="p">(</span><span class="n">executable</span><span class="o">.</span><span class="n">get_hlo_text</span><span class="p">())</span>

<span class="n">as_option</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">as_option_backup</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="force-to-use-data-parallel">
<h3>Force to use data parallel<a class="headerlink" href="#force-to-use-data-parallel" title="Permalink to this headline"></a></h3>
<p>Alpa can forcibly generates data parallel solution, or map a specific
mesh dimension to the batch dimension.</p>
<p>With force_batch_dim_to_mesh_dim, Alpa forcibly maps the given logical mesh
dimension (0 or 1) to batch dimension inferred in auto-sharding.
If the option’s value is None, but the two dimensions of the logical mesh is
larger than 1, Alpa still forcibly maps the first logical mesh dimension to
batch dimension.</p>
<p>With force_data_parallel, Alpa sets the first dimension larger than 1 to the force_batch_dim_to_mesh_dim value.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Default mesh shape: (num_host,num_device)=(1,4)</span>

<span class="n">as_option</span><span class="o">.</span><span class="n">force_batch_dim_to_mesh_dim</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">reset_state</span><span class="p">()</span>
<span class="n">executable</span> <span class="o">=</span> <span class="n">parallelize</span><span class="p">(</span><span class="n">train_step</span><span class="p">)</span><span class="o">.</span><span class="n">get_executable</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
<span class="n">print_hlo_communication_stats</span><span class="p">(</span><span class="n">executable</span><span class="o">.</span><span class="n">get_hlo_text</span><span class="p">())</span>
<span class="c1"># The above uses model parallel</span>

<span class="n">as_option</span><span class="o">.</span><span class="n">force_batch_dim_to_mesh_dim</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">reset_state</span><span class="p">()</span>
<span class="n">executable</span> <span class="o">=</span> <span class="n">parallelize</span><span class="p">(</span><span class="n">train_step</span><span class="p">)</span><span class="o">.</span><span class="n">get_executable</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
<span class="n">print_hlo_communication_stats</span><span class="p">(</span><span class="n">executable</span><span class="o">.</span><span class="n">get_hlo_text</span><span class="p">())</span>
<span class="c1"># The above uses data parallel</span>

<span class="n">as_option</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">as_option_backup</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="specify-inter-operator-parallelism-strategy">
<h2>Specify inter-operator parallelism strategy<a class="headerlink" href="#specify-inter-operator-parallelism-strategy" title="Permalink to this headline"></a></h2>
<p>We can specify inter-operator parallelism config with global_config.
To start with, we first set parallel strategy to 3d parallel and use alpa’s grad decorator:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">global_config</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
<span class="n">global_config</span><span class="o">.</span><span class="n">strategy</span> <span class="o">=</span> <span class="s2">&quot;pipeshard_parallel&quot;</span>
<span class="n">global_config</span><span class="o">.</span><span class="n">devices</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">get_virtual_physical_mesh</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">loss_func</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">])</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">out</span> <span class="o">-</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="c1"># modify the grad decorator here</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">alpa</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_func</span><span class="p">)(</span><span class="n">state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">new_state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads</span><span class="o">=</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_state</span>


<span class="k">def</span> <span class="nf">profile_and_pp_pipeshard_stats</span><span class="p">(</span><span class="n">executable</span><span class="p">):</span>
    <span class="n">pipeshard_stats</span> <span class="o">=</span> <span class="n">executable</span><span class="o">.</span><span class="n">profile_all_executables</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;All stages&#39; stats in form of (time, memory)&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">mesh_idx</span><span class="p">,</span> <span class="n">mesh_stats</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pipeshard_stats</span><span class="p">):</span>
        <span class="n">output_str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="k">for</span> <span class="n">stat</span> <span class="ow">in</span> <span class="n">mesh_stats</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">output_str</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;(</span><span class="si">{</span><span class="n">stat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">s,</span><span class="si">{</span><span class="n">stat</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">GB),&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mesh </span><span class="si">{</span><span class="n">mesh_idx</span><span class="si">}</span><span class="s2">:&quot;</span> <span class="o">+</span> <span class="n">output_str</span><span class="p">)</span>
</pre></div>
</div>
<section id="specify-layer-clustering">
<h3>Specify layer clustering<a class="headerlink" href="#specify-layer-clustering" title="Permalink to this headline"></a></h3>
<p>Layer cluster forms a number of JaxprEqns (atom in JAX IR) into the same layer.
We can also manually assign layers using the pipeline marker.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">alpa</span> <span class="kn">import</span> <span class="n">mark_pipeline</span><span class="p">,</span> <span class="n">manual_layer_construction</span>


<span class="k">class</span> <span class="nc">UnequalManualLayerBertLayerModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">BertConfig</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span>
    <span class="n">manual_pipeline_layer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">FlaxBertLayer</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="c1"># Add the pipeline start marker here</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">mark_pipeline</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">mark_type</span><span class="o">=</span><span class="s1">&#39;start&#39;</span><span class="p">)</span>
            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># Add the pipeline end marker here</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">mark_pipeline</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">mark_type</span><span class="o">=</span><span class="s1">&#39;end&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="c1"># Add the manual layer construction decorator here</span>
    <span class="nd">@manual_layer_construction</span><span class="p">(</span><span class="n">lift_markers</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">loss_func</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">])</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">out</span> <span class="o">-</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="n">alpa</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_func</span><span class="p">)(</span><span class="n">state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">new_state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads</span><span class="o">=</span><span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_state</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">UnequalManualLayerBertLayerModel</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">bert_config</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">create_train_state</span><span class="p">(</span><span class="n">rngkey</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">])</span>

<span class="n">executable</span> <span class="o">=</span> <span class="n">parallelize</span><span class="p">(</span><span class="n">train_step</span><span class="p">)</span><span class="o">.</span><span class="n">get_executable</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
<span class="n">profile_and_pp_pipeshard_stats</span><span class="p">(</span><span class="n">executable</span><span class="p">)</span>

<span class="n">executable</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
</pre></div>
</div>
<p>The code above creates a model with four bert layers, then split them into
two alpa layers. With default setting, each layer maps a pipeline stage and
each stage use the same submesh. As we split between the first bert layer and
the other three layers, the memory consumption of the first stage is
approximately third of the second’s.</p>
<p>In manual layer construction, each instruction in the forward computation
should between a pipeline start marker and its corresponding pipeline end
marker. When using the manual pipeline marker, the loss function should be
decorated by the manual_layer_construction mark.</p>
<p>For simplicity, manual_layer_construction provides a lift_marker option.
If it is turned on, the first and last pipeline marker are automatically
moved to the first and last JaxprEqn.</p>
</section>
<section id="specify-stage-construction">
<h3>Specify stage construction<a class="headerlink" href="#specify-stage-construction" title="Permalink to this headline"></a></h3>
<p>Stage construction merges layers into stages and assigns devices to each stage
with a logical mesh shape. Here we manually give the stage construction plan
with options in global_config.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EqualManualLayerBertLayerModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">BertConfig</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span>
    <span class="n">manual_pipeline_layer</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">FlaxBertLayer</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="c1"># Add the pipeline start marker here</span>
            <span class="n">mark_pipeline</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">mark_type</span><span class="o">=</span><span class="s1">&#39;start&#39;</span><span class="p">)</span>
            <span class="n">layer_outputs</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1"># Add the pipeline end marker here</span>
            <span class="n">mark_pipeline</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="n">mark_type</span><span class="o">=</span><span class="s1">&#39;end&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">EqualManualLayerBertLayerModel</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">bert_config</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">create_train_state</span><span class="p">(</span><span class="n">rngkey</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">])</span>

<span class="n">global_config_backup</span> <span class="o">=</span> <span class="n">global_config</span><span class="o">.</span><span class="n">backup</span><span class="p">()</span>

<span class="c1"># turn on manual stage plan</span>
<span class="n">global_config</span><span class="o">.</span><span class="n">pipeline_stage_mode</span> <span class="o">=</span> <span class="s2">&quot;manual_gpipe&quot;</span>
<span class="c1"># Layer-stage mapping</span>
<span class="n">global_config</span><span class="o">.</span><span class="n">forward_stage_layer_ids</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span>
<span class="c1"># Physical mesh shape of each stage</span>
<span class="n">global_config</span><span class="o">.</span><span class="n">sub_physical_mesh_shapes</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]</span>
<span class="c1"># Logical mesh shape of each stage</span>
<span class="n">global_config</span><span class="o">.</span><span class="n">sub_logical_mesh_shapes</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
<span class="c1"># auto sharding option of each stage</span>
<span class="n">global_config</span><span class="o">.</span><span class="n">submesh_autosharding_option_dicts</span> <span class="o">=</span> <span class="p">[{},</span> <span class="p">{},</span> <span class="p">{}]</span>
<span class="n">executable</span> <span class="o">=</span> <span class="n">parallelize</span><span class="p">(</span><span class="n">train_step</span><span class="p">)</span><span class="o">.</span><span class="n">get_executable</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
<span class="n">profile_and_pp_pipeshard_stats</span><span class="p">(</span><span class="n">executable</span><span class="p">)</span>

<span class="n">executable</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
<span class="n">global_config</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">global_config_backup</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="rematerialization-with-layer-construction">
<h3>Rematerialization with layer construction<a class="headerlink" href="#rematerialization-with-layer-construction" title="Permalink to this headline"></a></h3>
<p>We provide a layer-based rematerialization.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">EqualManualLayerBertLayerModel</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">bert_config</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">create_train_state</span><span class="p">(</span><span class="n">rngkey</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">get_train_step</span><span class="p">(</span><span class="n">remat_layer</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>

        <span class="c1"># Set remat_layer in manual layer construction decorator here.</span>
        <span class="c1"># The same is true for automatic layer construction decorator.</span>
        <span class="nd">@manual_layer_construction</span><span class="p">(</span><span class="n">lift_markers</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">remat_layer</span><span class="o">=</span><span class="n">remat_layer</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">loss_func</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">out</span> <span class="o">-</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">loss</span>

        <span class="n">grads</span> <span class="o">=</span> <span class="n">alpa</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_func</span><span class="p">)(</span><span class="n">state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
        <span class="n">new_state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads</span><span class="o">=</span><span class="n">grads</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_state</span>

    <span class="k">return</span> <span class="n">train_step</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&gt;&gt;&gt;&gt;&gt; With remat&quot;</span><span class="p">)</span>
<span class="n">executable</span> <span class="o">=</span> <span class="n">parallelize</span><span class="p">(</span><span class="n">get_train_step</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">get_executable</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
<span class="n">profile_and_pp_pipeshard_stats</span><span class="p">(</span><span class="n">executable</span><span class="p">)</span>
<span class="n">executable</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
<span class="n">reset_state</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&gt;&gt;&gt;&gt;&gt; Without remat&quot;</span><span class="p">)</span>
<span class="n">executable</span> <span class="o">=</span> <span class="n">parallelize</span><span class="p">(</span><span class="n">get_train_step</span><span class="p">(</span><span class="kc">False</span><span class="p">))</span><span class="o">.</span><span class="n">get_executable</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
<span class="n">profile_and_pp_pipeshard_stats</span><span class="p">(</span><span class="n">executable</span><span class="p">)</span>
<span class="n">executable</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
</pre></div>
</div>
<p>The peak memory is significantly smaller when remat_layer is turned on.</p>
<p>Moreover, we can remat at a fine-grained level, then do parallel at a relatively
coarse-grained level. The example below remat at each Bert Layer, but do
inter-operator parallelization for each two Bert Layers</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">alpa</span> <span class="kn">import</span> <span class="n">automatic_remat</span><span class="p">,</span> <span class="n">automatic_layer_construction</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">BertLayerModel</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">bert_config</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_train_step</span><span class="p">(</span><span class="n">remat_layer</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>

        <span class="k">def</span> <span class="nf">loss_func</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_fn</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">out</span> <span class="o">-</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">loss</span>

        <span class="c1"># Split the forward into 4 parts for remat</span>
        <span class="k">if</span> <span class="n">remat_layer</span><span class="p">:</span>
            <span class="n">loss_func</span> <span class="o">=</span> <span class="n">automatic_remat</span><span class="p">(</span><span class="n">loss_func</span><span class="p">,</span> <span class="n">layer_num</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
        <span class="c1"># Split the forward(remat-marked) into 2 parts for inter-operator parallel</span>
        <span class="n">loss_func</span> <span class="o">=</span> <span class="n">automatic_layer_construction</span><span class="p">(</span><span class="n">loss_func</span><span class="p">,</span> <span class="n">layer_num</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">alpa</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_func</span><span class="p">)(</span><span class="n">state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
        <span class="n">new_state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">grads</span><span class="o">=</span><span class="n">grads</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_state</span>

    <span class="k">return</span> <span class="n">train_step</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&gt;&gt;&gt;&gt;&gt; With remat&quot;</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">create_train_state</span><span class="p">(</span><span class="n">rngkey</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">])</span>
<span class="n">executable</span> <span class="o">=</span> <span class="n">parallelize</span><span class="p">(</span><span class="n">get_train_step</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">get_executable</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
<span class="n">profile_and_pp_pipeshard_stats</span><span class="p">(</span><span class="n">executable</span><span class="p">)</span>
<span class="n">executable</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
<span class="n">reset_state</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&gt;&gt;&gt;&gt;&gt; Without remat&quot;</span><span class="p">)</span>
<span class="n">executable</span> <span class="o">=</span> <span class="n">parallelize</span><span class="p">(</span><span class="n">get_train_step</span><span class="p">(</span><span class="kc">False</span><span class="p">))</span><span class="o">.</span><span class="n">get_executable</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
<span class="n">profile_and_pp_pipeshard_stats</span><span class="p">(</span><span class="n">executable</span><span class="p">)</span>
<span class="n">executable</span><span class="o">.</span><span class="n">shutdown</span><span class="p">()</span>
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  3.069 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-tutorials-advanced-api-usage-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/18304e704380c09c3625dcb9a55cf81f/advanced_api_usage.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">advanced_api_usage.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/e30a966b2c85535e818dde977520dcc0/advanced_api_usage.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">advanced_api_usage.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="alpa_vs_pmap.html" class="btn btn-neutral float-left" title="Difference between alpa.parallelize and jax.pmap" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>