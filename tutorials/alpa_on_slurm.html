<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using Alpa on Slurm &mdash; Alpa 0.2.2.dev47 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/alpa-logo.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Frequently Asked Questions (FAQ)" href="faq.html" />
    <link rel="prev" title="ICML’22 Big Model Tutorial" href="icml_big_model_tutorial.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Alpa
          </a>
              <div class="version">
                0.2.2.dev47
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install Alpa</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Alpa Quickstart</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="pipeshard_parallelism.html">Distributed Training with Both Shard and Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="alpa_vs_pmap.html">Differences between alpa.parallelize, jax.pmap and jax.pjit</a></li>
<li class="toctree-l1"><a class="reference internal" href="opt_serving.html">Serving OPT-175B, BLOOM-176B and CodeGen-16B using Alpa</a></li>
<li class="toctree-l1"><a class="reference internal" href="perf_tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="icml_big_model_tutorial.html">ICML’22 Big Model Tutorial</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Using Alpa on Slurm</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#prerequisites">Prerequisites</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#create-environment-for-alpa">Create Environment for Alpa</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#enter-interactive-mode">Enter Interactive Mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="#load-required-software">Load Required Software</a></li>
<li class="toctree-l4"><a class="reference internal" href="#install-and-check-dependencies">Install and Check Dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="#exit-virtual-environment">Exit Virtual Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="#exit-interactive-session">Exit Interactive Session</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#create-sbatch-script">Create <code class="code docutils literal notranslate"><span class="pre">sbatch</span></code> Script</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#resources-setup">Resources Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="#load-dependencies">Load Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ray-startup">Ray Startup</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optional-check-ray-is-running">[Optional] Check Ray is Running</a></li>
<li class="toctree-l3"><a class="reference internal" href="#run-alpa">Run Alpa</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#submit-job">Submit Job</a></li>
<li class="toctree-l2"><a class="reference internal" href="#check-output">Check Output</a></li>
<li class="toctree-l2"><a class="reference internal" href="#sample-sbatch-scripts">Sample <code class="code docutils literal notranslate"><span class="pre">sbatch`</span></code> Scripts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#multi-node-script">Multi-node Script</a></li>
<li class="toctree-l3"><a class="reference internal" href="#single-node-script">Single Node Script</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions (FAQ)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Benchmark</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/benchmark.html">Performance Benchmark</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Architecture</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture/overview.html">Design and Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/alpa_compiler_walk_through.html">Alpa Compiler Walk-Through</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/intra_op_solver.html">Code Structure of the Intra-op Solver</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../developer/developer_guide.html">Developer Guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Alpa</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Using Alpa on Slurm</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/alpa-projects/alpa/blob/main/docs/tutorials/alpa_on_slurm.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="using-alpa-on-slurm">
<h1>Using Alpa on Slurm<a class="headerlink" href="#using-alpa-on-slurm" title="Permalink to this headline"></a></h1>
<p>This page provides instructions to run Alpa on clusters managed by Slurm.
This guide is modified from <a class="reference external" href="https://docs.ray.io/en/latest/cluster/vms/user-guides/community/slurm.html">Deploy Ray on Slurm</a>.</p>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline"></a></h2>
<p>Alpa requires CUDA and runs on GPUs. Therefore, you need access to CUDA-compatible GPUs in the cluster to run Alpa.</p>
<p>If you have a virtual environment with required dependencies for Alpa installed, please jump to the section <a class="reference internal" href="#create-sbatch-script">Create sbatch Script</a>.</p>
<p>If you don’t have one already, please follow the steps below to create a Python virtual environment for Alpa.</p>
<section id="create-environment-for-alpa">
<h3>Create Environment for Alpa<a class="headerlink" href="#create-environment-for-alpa" title="Permalink to this headline"></a></h3>
<p>Here we give a sample workflow to create an environment for Alpa and verfy the environment as well.</p>
<p>We assume that there is Python environment management systems available on the Slurm cluster, e.g. <a class="reference external" href="https://docs.conda.io/en/latest/">conda</a>.</p>
<p>The following steps can make sure you set up the environment needed by Alpa correctly. To illustrate this process we use conda, but the workflow also applies to other Python virtual environment management systems.</p>
<section id="enter-interactive-mode">
<h4>Enter Interactive Mode<a class="headerlink" href="#enter-interactive-mode" title="Permalink to this headline"></a></h4>
<p>The first step is to create an interactive mode on Slurm so that you can run commands and check results in real-time.</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>interact -p GPU-shared -N <span class="m">1</span> --gres<span class="o">=</span>gpu:v100-16:1 -t <span class="m">10</span>:00
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic simple">
<li><p>This command starts an interactive job on the partition of GPU-shared (<code class="code docutils literal notranslate"><span class="pre">-p</span> <span class="pre">GPU-shared</span></code>), on one GPU node (<code class="code docutils literal notranslate"><span class="pre">-N</span> <span class="pre">1</span></code>) with one v100-16GB GPU assigned (<code class="code docutils literal notranslate"><span class="pre">--gres=gpu:v100-16:1</span></code>) for a time of 10 minutes (<code class="code docutils literal notranslate"><span class="pre">-t</span> <span class="pre">10:00</span></code>).</p></li>
<li><p>The minimum command to start an interactive job is by running <code class="code docutils literal notranslate"><span class="pre">interact</span></code> directly. This will ask the cluster to assign all resources by default. As we want to setup and test the environment, we need GPU to test and hence we need to specify all these options.</p></li>
<li><p>The name of the partition, the option for assigning GPU (in some clusters, <code class="code docutils literal notranslate"><span class="pre">--gpus=</span></code> is used instead of <code class="code docutils literal notranslate"><span class="pre">--gres=</span></code>), and the name for GPU depend on the cluster you work on. Please use these values specified by your cluster.</p></li>
</ol>
</div>
</div></blockquote>
<p>Then the cluster will try to assign the resources you asked for.</p>
<p>Once you got these resources, the command <code class="code docutils literal notranslate"><span class="pre">interact</span></code> returns and you are now in an interactive session. Your commandline will show that you are now at the node of the cluster like <code class="code docutils literal notranslate"><span class="pre">user&#64;v001:~/$</span></code> where <code class="code docutils literal notranslate"><span class="pre">v001</span></code> is the name of the node you are assigned to.</p>
</section>
<section id="load-required-software">
<h4>Load Required Software<a class="headerlink" href="#load-required-software" title="Permalink to this headline"></a></h4>
<p>In some clusters, containers of common tools are packed and can be loaded through <code class="code docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">&lt;target&gt;</span></code>.</p>
<p>For example, to run Alpa, we need CUDA, so we run the following to get CUDA:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module load cuda
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some clusters have multiple versions of CUDA for you to use, you can check all versions using:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module avail cuda
</pre></div>
</div>
</div></blockquote>
<p>Then you can specify a version to be used. Like Alpa requires CUDA &gt;= 11.1.</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module load cuda/11.1
</pre></div>
</div>
</div></blockquote>
<p>Please refer to the manager of the cluster to install a specific versions of CUDA or other softwares if not available.</p>
</div>
</div></blockquote>
<p>Similarly, we can load CuDNN and NVCC:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module load cudnn
module load nvhpc
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic simple">
<li><p>Note here <code class="code docutils literal notranslate"><span class="pre">nvhpc</span></code> is loaded as it contains <code class="code docutils literal notranslate"><span class="pre">nvcc</span></code> in the cluster. <code class="code docutils literal notranslate"><span class="pre">nvcc</span></code> might need to be loaded differently in other clusters. Please check with your cluster on how to load <code class="code docutils literal notranslate"><span class="pre">nvcc</span></code>.</p></li>
<li><p>A common issue with CuDNN on clusters is the version provided by the cluster is older than the version required by Alpa (&gt;= 8.0.5). To solve this, one can install a compatible version of CuDNN inside the Python virtual environment like the installation of other dependencies covered in <a class="reference internal" href="#install-and-check-dependencies">Install and Check Dependencies</a>.</p></li>
</ol>
</div>
</div></blockquote>
</section>
<section id="install-and-check-dependencies">
<h4>Install and Check Dependencies<a class="headerlink" href="#install-and-check-dependencies" title="Permalink to this headline"></a></h4>
<p>Before you install dependencies of Alpa, create a virtual environment with the version of Python you use:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda create -n alpa_environment <span class="nv">python</span><span class="o">=</span><span class="m">3</span>.9
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please make sure the Python version meets the requirement by Alpa of &gt;= 3.7.</p>
</div>
</div></blockquote>
<p>Then enter this Python virtual environment:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda activate alpa_environment
</pre></div>
</div>
</div></blockquote>
<p>Then your commandline should show as <code class="code docutils literal notranslate"><span class="pre">(alpa_environment)user&#64;v001:~/$</span></code>.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Check the environment is entered by checking the version of python:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 --version
</pre></div>
</div>
</div></blockquote>
</div>
</div></blockquote>
<p>Then please follow the installation guide of Alpa in <a class="reference external" href="https://alpa.ai/install.html#install-alpa">Install Alpa</a>.
All the commands mentioned in the installation guide works for you and can make sure the environment <code class="code docutils literal notranslate"><span class="pre">alpa_environment</span></code> works with Alpa.</p>
</section>
<section id="exit-virtual-environment">
<h4>Exit Virtual Environment<a class="headerlink" href="#exit-virtual-environment" title="Permalink to this headline"></a></h4>
<p>Once you have finished installation and testing, exit the environment:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda deactivate
</pre></div>
</div>
</div></blockquote>
<p>Next time you want to activate this environment, use the following command:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda activate alpa_environment
</pre></div>
</div>
</div></blockquote>
</section>
<section id="exit-interactive-session">
<h4>Exit Interactive Session<a class="headerlink" href="#exit-interactive-session" title="Permalink to this headline"></a></h4>
<p>To exit interactive session, press <code class="code docutils literal notranslate"><span class="pre">Ctrl+D</span></code>.</p>
</section>
</section>
</section>
<section id="create-sbatch-script">
<h2>Create <code class="code docutils literal notranslate"><span class="pre">sbatch</span></code> Script<a class="headerlink" href="#create-sbatch-script" title="Permalink to this headline"></a></h2>
<p>Usually large jobs like Alpa is run through sbatch on Slurm using a <code class="code docutils literal notranslate"><span class="pre">sbatch</span></code> script. <code class="code docutils literal notranslate"><span class="pre">sbatch</span></code> scripts are bash scripts with <code class="code docutils literal notranslate"><span class="pre">sbatch</span></code> options specified using the syntax of <code class="code docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">&lt;options&gt;</span></code>.
The Slurm cluster takes sbatch scripts submitted using command <code class="code docutils literal notranslate"><span class="pre">sbatch`</span></code> then queues the job specified by the script for execution.
When Slurm executes the script, the script works exactly the same as a shell script.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The shell script commands are run on each of the nodes assigned for your job. To specify running a command at one node, use command <code class="code docutils literal notranslate"><span class="pre">srun</span></code>’s option of <code class="code docutils literal notranslate"><span class="pre">--nodes=1</span></code>.
Available options for <code class="code docutils literal notranslate"><span class="pre">srun</span></code> can be found in <a class="reference external" href="https://slurm.schedmd.com/srun.html">SRUN</a>. <code class="code docutils literal notranslate"><span class="pre">srun</span></code> is to run a job for execution in real time while <code class="code docutils literal notranslate"><span class="pre">sbatch</span></code> allows you to submit a job for later execution without blocking.
<code class="code docutils literal notranslate"><span class="pre">srun</span></code> is also compatible with the script we create.</p>
</div>
</div></blockquote>
<p>A <code class="code docutils literal notranslate"><span class="pre">sbatch</span></code> script to run Alpa can be roughly summarized as four parts: resources setup, load dependencies, Ray startup, and run Alpa.</p>
<p>The first step is to create a <code class="code docutils literal notranslate"><span class="pre">sbatch</span></code> script in your directory, usually named as a <code class="code docutils literal notranslate"><span class="pre">.sh</span></code> file.
Here, this guide asumes the script is included in a file <code class="code docutils literal notranslate"><span class="pre">run_alpa_on_slurm.sh</span></code>.
Just like a shell script, the <code class="code docutils literal notranslate"><span class="pre">sbatch</span></code> script starts with a line specifying the path to interpreter:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
</pre></div>
</div>
</div></blockquote>
<section id="resources-setup">
<h3>Resources Setup<a class="headerlink" href="#resources-setup" title="Permalink to this headline"></a></h3>
<p>The first lines in your sbatch script is used to specify resources for the job like all the options you specify when running srun or sbatch.
These usually includes the name of the job, partition the job should go to, CPU per task, memory per CPU, number of nodes, number of tasks per node, and time limit for the job.</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --job-name=alpa_multinode_test</span>
<span class="c1">#SBATCH --partition=GPU</span>
<span class="c1">#SBATCH --nodes=2</span>
<span class="c1">#SBATCH --tasks-per-node=1</span>
<span class="c1">#SBATCH --cpus-per-task=1</span>
<span class="c1">#SBATCH --mem-per-cpu=1GB</span>
<span class="c1">#SBATCH --gpus-per-node=v100-16:8</span>
<span class="c1">#SBATCH --time=00:10:00</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic simple">
<li><p>Setting the resources needed in the sbatch script is equivalent to setting them submitting the job to Slurm running <code class="code docutils literal notranslate"><span class="pre">sbatch</span> <span class="pre">&lt;options&gt;</span> <span class="pre">&lt;sbatch</span> <span class="pre">script&gt;</span></code>.</p></li>
<li><p>Here, <code class="code docutils literal notranslate"><span class="pre">--tasks-per-node=1</span> <span class="pre">--cpus-per-task=1</span></code> are specified to allow Ray (which uses CPU) to run on the nodes.</p></li>
<li><p>The option <a href="#id1"><span class="problematic" id="id2">:node:`--gpus-per-node=v100-16:8`</span></a> is specified with GPU type and number. Please refer to your cluster on how to set this field.</p></li>
</ol>
</div>
</div></blockquote>
</section>
<section id="load-dependencies">
<h3>Load Dependencies<a class="headerlink" href="#load-dependencies" title="Permalink to this headline"></a></h3>
<p>The next step is to setup the environment with Alpa’s dependencies installed.
In some Slurm clusters, CUDA, NVCC, and CuDNN are packed in containers that can be loaded directly. Here, we provide an example that loads a combination of available container and user-defined environment from package management systems.
To load directly from available containers, use <code class="code docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">&lt;module&gt;</span></code>:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module load cuda
module load cudnn
module load nvhpc
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic">
<li><p>To check available softwares, run:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module avail
module avail cuda
</pre></div>
</div>
</div></blockquote>
</li>
</ol>
<p>When there is no required software, please ask manager of the cluster to install.</p>
<p>When multiple versions available, one can specify the version to be used:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module load cuda/11.1.1
module load cudnn/8.0.5
</pre></div>
</div>
</div></blockquote>
<p>You can check the module needed is used with:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>module list cuda
</pre></div>
</div>
</div></blockquote>
<ol class="arabic simple">
<li><p>The load of pre-installed software can be different in different clusters, please use the way your cluster uses.</p></li>
</ol>
</div>
</div></blockquote>
<p>To activate an environment using package management systems like conda, add the following line:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda activate alpa_environment
</pre></div>
</div>
</div></blockquote>
<p>In summary, this step adds a chunk in the script like below:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># load containers</span>
module purge    <span class="c1"># optional</span>
module load cuda
module load cudnn
module load nvhpc
<span class="c1"># activate conda environment</span>
conda activate alpa_environment
</pre></div>
</div>
</div></blockquote>
<p>After this step, all the dependencies, including packages and softwares needed for Alpa is loaded and can be used.</p>
<p>Running within one node in the cluster, you can jump to use <a class="reference internal" href="#single-node-script">Single Node Script</a>.</p>
</section>
<section id="ray-startup">
<h3>Ray Startup<a class="headerlink" href="#ray-startup" title="Permalink to this headline"></a></h3>
<p>Then it’s time for Ray to run.
The first step is to grab the nodes assigned in the cluster to this job and name one node to be head node in the topology of a Ray cluster:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get names of nodes assigned</span>
<span class="nv">nodes</span><span class="o">=</span><span class="k">$(</span>scontrol show hostnames <span class="s2">&quot;</span><span class="nv">$SLURM_JOB_NODELIST</span><span class="s2">&quot;</span><span class="k">)</span>
<span class="nv">nodes_array</span><span class="o">=(</span><span class="nv">$nodes</span><span class="o">)</span>

<span class="c1"># By default, set the first node to be head_node on which we run HEAD of Ray</span>
<span class="nv">head_node</span><span class="o">=</span><span class="si">${</span><span class="nv">nodes_array</span><span class="p">[0]</span><span class="si">}</span>
<span class="nv">head_node_ip</span><span class="o">=</span><span class="k">$(</span>srun --nodes<span class="o">=</span><span class="m">1</span> --ntasks<span class="o">=</span><span class="m">1</span> -w <span class="s2">&quot;</span><span class="nv">$head_node</span><span class="s2">&quot;</span> hostname --ip-address<span class="k">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case of a cluster uses ipv6 addresses, one can use the following script after we get head node ip to change it to ipv4:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert ipv6 address to ipv4 address</span>
<span class="k">if</span> <span class="o">[[</span> <span class="s2">&quot;</span><span class="nv">$head_node_ip</span><span class="s2">&quot;</span> <span class="o">==</span> *<span class="s2">&quot; &quot;</span>* <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
<span class="nv">IFS</span><span class="o">=</span><span class="s1">&#39; &#39;</span> <span class="nb">read</span> -ra ADDR <span class="o">&lt;&lt;&lt;</span><span class="s2">&quot;</span><span class="nv">$head_node_ip</span><span class="s2">&quot;</span>
<span class="k">if</span> <span class="o">[[</span> <span class="si">${#</span><span class="nv">ADDR</span><span class="p">[0]</span><span class="si">}</span> -gt <span class="m">16</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then</span>
<span class="nv">head_node_ip</span><span class="o">=</span><span class="si">${</span><span class="nv">ADDR</span><span class="p">[1]</span><span class="si">}</span>
<span class="k">else</span>
<span class="nv">head_node_ip</span><span class="o">=</span><span class="si">${</span><span class="nv">ADDR</span><span class="p">[0]</span><span class="si">}</span>
<span class="k">fi</span>
<span class="nb">echo</span> <span class="s2">&quot;Found IPV6 address, split the IPV4 address as </span><span class="nv">$head_node_ip</span><span class="s2">&quot;</span>
<span class="k">fi</span>
</pre></div>
</div>
</div>
</div></blockquote>
<p>After we have a head node, we spawn HEAD on head node:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup port and variables needed</span>
<span class="nv">gpus_per_node</span><span class="o">=</span><span class="m">8</span>
<span class="nv">port</span><span class="o">=</span><span class="m">6789</span>
<span class="nv">ip_head</span><span class="o">=</span><span class="nv">$head_node_ip</span>:<span class="nv">$port</span>
<span class="nb">export</span> ip_head
<span class="c1"># Start HEAD in background of head node</span>
srun --nodes<span class="o">=</span><span class="m">1</span> --ntasks<span class="o">=</span><span class="m">1</span> -w <span class="s2">&quot;</span><span class="nv">$head_node</span><span class="s2">&quot;</span> <span class="se">\</span>
        ray start --head --node-ip-address<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$head_node_ip</span><span class="s2">&quot;</span> --port<span class="o">=</span><span class="nv">$port</span> <span class="se">\</span>
        --num-cpus <span class="s2">&quot;</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="si">}</span><span class="s2">&quot;</span> --num-gpus <span class="nv">$gpus_per_node</span> --block <span class="p">&amp;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note here the argument <code class="code docutils literal notranslate"><span class="pre">gpus_per_node</span></code> should not exceed the number of GPU you have on each node.</p>
</div>
</div></blockquote>
<p>Then we spawn worker nodes on other nodes we have and connect them to HEAD:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start worker nodes</span>
<span class="c1"># Number of nodes other than the head node</span>
<span class="nv">worker_num</span><span class="o">=</span><span class="k">$((</span>SLURM_JOB_NUM_NODES <span class="o">-</span> <span class="m">1</span><span class="k">))</span>
<span class="c1"># Iterate on each node other than head node, start ray worker and connect to HEAD</span>
<span class="k">for</span> <span class="o">((</span><span class="nv">i</span> <span class="o">=</span> <span class="m">1</span><span class="p">;</span> i &lt;<span class="o">=</span> worker_num<span class="p">;</span> i++<span class="o">))</span><span class="p">;</span> <span class="k">do</span>
    <span class="nv">node_i</span><span class="o">=</span><span class="si">${</span><span class="nv">nodes_array</span><span class="p">[</span><span class="nv">$i</span><span class="p">]</span><span class="si">}</span>
    <span class="nb">echo</span> <span class="s2">&quot;Starting WORKER </span><span class="nv">$i</span><span class="s2"> at </span><span class="nv">$node_i</span><span class="s2">&quot;</span>
    srun --nodes<span class="o">=</span><span class="m">1</span> --ntasks<span class="o">=</span><span class="m">1</span> -w <span class="s2">&quot;</span><span class="nv">$node_i</span><span class="s2">&quot;</span> <span class="se">\</span>
        ray start --address <span class="s2">&quot;</span><span class="nv">$ip_head</span><span class="s2">&quot;</span> --num-cpus <span class="s2">&quot;</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="si">}</span><span class="s2">&quot;</span> <span class="se">\</span>
        --num-gpus <span class="nv">$gpus_per_node</span> --block <span class="p">&amp;</span>
    sleep <span class="m">5</span>
<span class="k">done</span>
</pre></div>
</div>
</div></blockquote>
</section>
<section id="optional-check-ray-is-running">
<h3>[Optional] Check Ray is Running<a class="headerlink" href="#optional-check-ray-is-running" title="Permalink to this headline"></a></h3>
<p>You can check if Ray is started and all nodes connected by adding this line:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ray list nodes
</pre></div>
</div>
</div></blockquote>
<p>In the output of the job, you are expected to see the same number of nodes you asked for listed by this command.</p>
<p>At this time, we have a running Ray instance and next we can run Alpa on it.</p>
</section>
<section id="run-alpa">
<h3>Run Alpa<a class="headerlink" href="#run-alpa" title="Permalink to this headline"></a></h3>
<p>Just like running Alpa locally, the previous steps are equivalent of having run ray with <code class="code docutils literal notranslate"><span class="pre">ray</span> <span class="pre">start</span> <span class="pre">--head</span></code>.
Then it’s time to run Alpa:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python3 -m alpa.test_install
</pre></div>
</div>
</div></blockquote>
</section>
</section>
<section id="submit-job">
<h2>Submit Job<a class="headerlink" href="#submit-job" title="Permalink to this headline"></a></h2>
<p>To submit the job, run the following command:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch run_alpa_on_slurm.sh
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>After you submit the job, Slurm will tell you the job’s number. You can check your submitted job’s status using command squeue.
To find all jobs you have, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>squeue -u &lt;your_user_name&gt;
</pre></div>
</div>
<p>To check all jobs running and queued in a partition, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>squeue -p &lt;partition_name&gt;
</pre></div>
</div>
<p>When you no longer see a job in the list, it means the job is finished.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Another option is to run with <code class="code docutils literal notranslate"><span class="pre">srun</span></code>.
This will give an interactive session, meaning the output will show up in your terminal
while <code class="code docutils literal notranslate"><span class="pre">sbatch</span></code> collects output to a file.</p>
<p>Run with <code class="code docutils literal notranslate"><span class="pre">srun</span></code> is exactly the same as <code class="code docutils literal notranslate"><span class="pre">sbatch</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun run_alpa_on_slurm.sh
</pre></div>
</div>
</div>
</div></blockquote>
</section>
<section id="check-output">
<h2>Check Output<a class="headerlink" href="#check-output" title="Permalink to this headline"></a></h2>
<p>After a Slurm job is finished, the output will appear in your directory as a file (if you submitted the job through <code class="code docutils literal notranslate"><span class="pre">sbatch</span></code>).
On some Slurm clusters, the output file is named <code class="code docutils literal notranslate"><span class="pre">slurm-&lt;job_number&gt;.out</span></code>.
You can check the file for output the same way you read a text file.</p>
</section>
<section id="sample-sbatch-scripts">
<h2>Sample <code class="code docutils literal notranslate"><span class="pre">sbatch`</span></code> Scripts<a class="headerlink" href="#sample-sbatch-scripts" title="Permalink to this headline"></a></h2>
<section id="multi-node-script">
<h3>Multi-node Script<a class="headerlink" href="#multi-node-script" title="Permalink to this headline"></a></h3>
<p>Putting things together, a sample sbatch script that runs Alpa is as follows:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --job-name=alpa_multinode_test</span>
<span class="c1">#SBATCH --partition=GPU</span>
<span class="c1">#SBATCH --nodes=2</span>
<span class="c1">#SBATCH --tasks-per-node=1</span>
<span class="c1">#SBATCH --cpus-per-task=1</span>
<span class="c1">#SBATCH --mem-per-cpu=1GB</span>
<span class="c1">#SBATCH --gpus-per-node=v100-16:8</span>
<span class="c1">#SBATCH --time=00:10:00</span>

<span class="c1"># load containers</span>
module purge
module load cuda
module load cudnn
module load nvhpc
<span class="c1"># activate conda environment</span>
conda activate alpa_environment

<span class="c1"># Get names of nodes assigned</span>
<span class="nv">nodes</span><span class="o">=</span><span class="k">$(</span>scontrol show hostnames <span class="s2">&quot;</span><span class="nv">$SLURM_JOB_NODELIST</span><span class="s2">&quot;</span><span class="k">)</span>
<span class="nv">nodes_array</span><span class="o">=(</span><span class="nv">$nodes</span><span class="o">)</span>

<span class="c1"># By default, set the first node to be head_node on which we run HEAD of Ray</span>
<span class="nv">head_node</span><span class="o">=</span><span class="si">${</span><span class="nv">nodes_array</span><span class="p">[0]</span><span class="si">}</span>
<span class="nv">head_node_ip</span><span class="o">=</span><span class="k">$(</span>srun --nodes<span class="o">=</span><span class="m">1</span> --ntasks<span class="o">=</span><span class="m">1</span> -w <span class="s2">&quot;</span><span class="nv">$head_node</span><span class="s2">&quot;</span> hostname --ip-address<span class="k">)</span>

<span class="c1"># Setup port and variables needed</span>
<span class="nv">gpus_per_node</span><span class="o">=</span><span class="m">8</span>
<span class="nv">port</span><span class="o">=</span><span class="m">6789</span>
<span class="nv">ip_head</span><span class="o">=</span><span class="nv">$head_node_ip</span>:<span class="nv">$port</span>
<span class="nb">export</span> ip_head
<span class="c1"># Start HEAD in background of head node</span>
srun --nodes<span class="o">=</span><span class="m">1</span> --ntasks<span class="o">=</span><span class="m">1</span> -w <span class="s2">&quot;</span><span class="nv">$head_node</span><span class="s2">&quot;</span> <span class="se">\</span>
        ray start --head --node-ip-address<span class="o">=</span><span class="s2">&quot;</span><span class="nv">$head_node_ip</span><span class="s2">&quot;</span> --port<span class="o">=</span><span class="nv">$port</span> <span class="se">\</span>
        --num-cpus <span class="s2">&quot;</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="si">}</span><span class="s2">&quot;</span> --num-gpus <span class="nv">$gpus_per_node</span> --block <span class="p">&amp;</span>

<span class="c1"># Optional, sometimes needed for old Ray versions</span>
sleep <span class="m">10</span>

<span class="c1"># Start worker nodes</span>
<span class="c1"># Number of nodes other than the head node</span>
<span class="nv">worker_num</span><span class="o">=</span><span class="k">$((</span>SLURM_JOB_NUM_NODES <span class="o">-</span> <span class="m">1</span><span class="k">))</span>
<span class="c1"># Iterate on each node other than head node, start ray worker and connect to HEAD</span>
<span class="k">for</span> <span class="o">((</span><span class="nv">i</span> <span class="o">=</span> <span class="m">1</span><span class="p">;</span> i &lt;<span class="o">=</span> worker_num<span class="p">;</span> i++<span class="o">))</span><span class="p">;</span> <span class="k">do</span>
    <span class="nv">node_i</span><span class="o">=</span><span class="si">${</span><span class="nv">nodes_array</span><span class="p">[</span><span class="nv">$i</span><span class="p">]</span><span class="si">}</span>
    <span class="nb">echo</span> <span class="s2">&quot;Starting WORKER </span><span class="nv">$i</span><span class="s2"> at </span><span class="nv">$node_i</span><span class="s2">&quot;</span>
    srun --nodes<span class="o">=</span><span class="m">1</span> --ntasks<span class="o">=</span><span class="m">1</span> -w <span class="s2">&quot;</span><span class="nv">$node_i</span><span class="s2">&quot;</span> <span class="se">\</span>
        ray start --address <span class="s2">&quot;</span><span class="nv">$ip_head</span><span class="s2">&quot;</span> --num-cpus <span class="s2">&quot;</span><span class="si">${</span><span class="nv">SLURM_CPUS_PER_TASK</span><span class="si">}</span><span class="s2">&quot;</span> <span class="se">\</span>
        --num-gpus <span class="nv">$gpus_per_node</span> --block <span class="p">&amp;</span>
    sleep <span class="m">5</span>
<span class="k">done</span>

<span class="c1"># Run Alpa test</span>
python3 -m alpa.test_install

<span class="c1"># Optional. Slurm will terminate all processes automatically</span>
ray stop
conda deactivate
<span class="nb">exit</span>
</pre></div>
</div>
</div></blockquote>
</section>
<section id="single-node-script">
<h3>Single Node Script<a class="headerlink" href="#single-node-script" title="Permalink to this headline"></a></h3>
<p>For running Alpa on Slurm with only one node or shared node, the following script can be used:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --job-name=alpa_uninode_test</span>
<span class="c1">#SBATCH -p GPU-shared</span>
<span class="c1">#SBATCH -N 1</span>
<span class="c1">#SBATCH --gpus=v100-16:1</span>
<span class="c1">#SBATCH -t 0:10:00</span>

<span class="c1"># load containers</span>
module purge
module load cuda
module load cudnn
module load nvhpc
<span class="c1"># activate conda environment</span>
conda activate alpa_environment

<span class="c1"># Start Ray on HEAD</span>
ray start --head

<span class="c1"># Run Alpa test</span>
python3 -m alpa.test_install

<span class="c1"># Optional. Slurm will terminate all processes automatically</span>
ray stop
conda deactivate
<span class="nb">exit</span>
</pre></div>
</div>
</div></blockquote>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="icml_big_model_tutorial.html" class="btn btn-neutral float-left" title="ICML’22 Big Model Tutorial" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="faq.html" class="btn btn-neutral float-right" title="Frequently Asked Questions (FAQ)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Alpa Developers.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-587CCSSRL2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-587CCSSRL2', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>