<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Design and Architecture &mdash; Alpa 0.2.0.dev8 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-binder.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-dataframe.css" type="text/css" />
      <link rel="stylesheet" href="../_static/sg_gallery-rendered-html.css" type="text/css" />
    <link rel="shortcut icon" href="../_static/alpa-logo.ico"/>
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Alpa Compiler Walk-Through" href="alpa_compiler_walk_through.html" />
    <link rel="prev" title="Performance Benchmark" href="../benchmark/benchmark.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Alpa
          </a>
              <div class="version">
                0.2.0.dev8
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install Alpa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/quickstart.html">Alpa Quickstart</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/pipeshard_parallelism.html">Distributed Training with Both Shard and Pipeline Parallelism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/alpa_vs_pmap.html">Differences between alpa.parallelize, jax.pmap and jax.pjit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/perf_tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/opt_serving.html">Serving OPT-175B using Alpa</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/icml_big_model_tutorial.html">ICML’22 Big Model Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/faq.html">Frequently Asked Questions (FAQ)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Benchmark</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/benchmark.html">Performance Benchmark</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Architecture</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Design and Architecture</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#compilation">Compilation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#important-concepts">Important concepts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#computational-graph">Computational graph</a></li>
<li class="toctree-l4"><a class="reference internal" href="#device-cluster">Device cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="#device-mesh">Device mesh</a></li>
<li class="toctree-l4"><a class="reference internal" href="#worker">Worker</a></li>
<li class="toctree-l4"><a class="reference internal" href="#stage">Stage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#resharding">Resharding</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#compilation-passes">Compilation Passes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#inter-op-pass">Inter-op Pass</a></li>
<li class="toctree-l4"><a class="reference internal" href="#intra-op-pass">Intra-op pass</a></li>
<li class="toctree-l4"><a class="reference internal" href="#runtime-orchestratoin-pass">Runtime Orchestratoin pass</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#runtime">Runtime</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="alpa_compiler_walk_through.html">Alpa Compiler Walk-Through</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../developer/developer_guide.html">Developer Guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Alpa</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Design and Architecture</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/alpa-projects/alpa/blob/main/docs/architecture/overview.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="design-and-architecture">
<h1>Design and Architecture<a class="headerlink" href="#design-and-architecture" title="Permalink to this headline"></a></h1>
<p>This document aims to describe the architecture of Alpa and explain several core concepts and compilation passes introduced by Alpa at a high level. It provides an overview of Alpa’s architecture, including core terms and componenents introduced by Alpa. In <a class="reference internal" href="alpa_compiler_walk_through.html#alpa-compiler-walk-through"><span class="std std-ref">Alpa Compiler Walk-Through</span></a>, we further show the workflow of Alpa using an MLP example.</p>
<p>You are recommended to read the the following materials as well:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/2201.12023.pdf">Alpa paper</a> (OSDI’22)</p></li>
<li><p><a class="reference external" href="https://ai.googleblog.com/2022/05/alpa-automated-model-parallel-deep.html">Google AI blog</a></p></li>
<li><p><a class="reference external" href="https://docs.google.com/presentation/d/1CQ4S1ff8yURk9XmL5lpQOoMMlsjw4m0zPS6zYDcyp7Y/edit?usp=sharing">Alpa talk slides</a></p></li>
</ul>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p><a class="reference internal" href="#architecture"><span class="std std-ref">The figure below</span></a> shows a high-level diagram of Alpa’s architecture.</p>
<figure class="align-center" id="id5">
<span id="architecture"></span><a class="reference internal image-reference" href="../_images/alpa-arch.png"><img alt="../_images/alpa-arch.png" src="../_images/alpa-arch.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-text">Figure 1: Alpa architecture diagram.</span><a class="headerlink" href="#id5" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Like many existing machine learning compilers, Alpa parallelizes the ML computation in two steps: a compilation step, followed by a runtime step.</p>
<p>In the compilation step, Alpa takes a model description, in the form of a <a class="reference internal" href="#cg"><span class="std std-ref">computational graph</span></a>, and a <a class="reference internal" href="#device-cluster"><span class="std std-ref">device cluster</span></a> as inputs, and performs a few compilation passes and optimizations to generate
a model-parallel execution plan, which is <em>custom-made</em> for the model and cluster. Alpa then generates binary executables based on the training code and parallel execution plan, for each parcipating compute device in the cluster.
In the runtime step, Alpa orchestrates the parallel execution of these executables on the cluster.</p>
</section>
<section id="compilation">
<h2>Compilation<a class="headerlink" href="#compilation" title="Permalink to this headline"></a></h2>
<p>Before we start introducing the compilation architecture, we bring in two important concepts introduced by Alpa.
Unlike many existing distributed ML training systems, Alpa views existing ML parallelization approaches into two orthogonal categories:
<strong>intra-operator parallelism</strong> and <strong>inter-operator parallelism</strong>. They are distinguished by the fact that if the parallelism approach involves partitioning any computational operator of the model along one (or more) tensor axis.
Some examples falling into the two categories are listed below:</p>
<ul class="simple">
<li><p><strong>Intra-op parallelism</strong>: data parallelism, Megatron-LM’s tensor model parallelism, operator parallelism such as those in ToFu and FlexFlow, etc.</p></li>
<li><p><strong>Inter-op parallelism</strong>: device placement, pipeline parallelism and their variants.</p></li>
</ul>
<p>For a deeper dive into what these two classes of parallelism entail, please read the documentation about our rationale.</p>
<p>This new view of ML parallelization techniques is the core part that drives Alpa’s design: Alpa unifies existing ML parallelization methods following this
view by realizing them in a two-level hierarchy shown in <a class="reference internal" href="#architecture"><span class="std std-ref">Figure 1</span></a>. At the upper level, Alpa designs a set of algorithms and compilation passes, which we call
<strong>inter-op pass</strong> to generate parallel execution plan corresponding to all inter-op parallelisms; at the lower level, Alpa designs another set of algorithms and
compilation passes, which we call <strong>intra-op pass</strong>, to generate the parallel execution plan mapping to all intra-op parallelisms.</p>
<p>Alpa can guarantee the plan generated at each individual level is <em>locally optimal</em>.
Once the two-level plans are generated, Alpa runs a third pass <strong>runtime orchestration pass</strong>. In this pass, Alpa applies the plans on the input computational graph,
performs some post-processing, and finally compile the original, single-node graph into parallel executables. It then sends the parallel executables to devices on the cluster.</p>
<section id="important-concepts">
<h3>Important concepts<a class="headerlink" href="#important-concepts" title="Permalink to this headline"></a></h3>
<p>Understanding the following concepts are necessary to understand what each pass is precisely doing during compilation.</p>
<section id="computational-graph">
<span id="cg"></span><h4>Computational graph<a class="headerlink" href="#computational-graph" title="Permalink to this headline"></a></h4>
<p>Like many machine learning compiler systems, Alpa represents the model computation as a static computational graph.
For now, this computational graph is first extracted from the user code and expressed using the <a class="reference external" href="https://jax.readthedocs.io/en/latest/jaxpr.html">JaxPR intermediate representation</a>,
and then lowered to the <a class="reference external" href="https://www.tensorflow.org/xla/operation_semantics">XLA HLO representation</a>.</p>
</section>
<section id="device-cluster">
<span id="id1"></span><h4>Device cluster<a class="headerlink" href="#device-cluster" title="Permalink to this headline"></a></h4>
<p>Alpa runs on a cluster of compute devices, managed by <a class="reference external" href="https://github.com/ray-project/ray">Ray</a>. For example, a cluster of four AWS p3.16xlarge nodes, with 8 GPUs on each node, form an 4x8 device cluster, illustrated
in <a class="reference internal" href="#cluster-mesh"><span class="std std-ref">Figure 2</span></a> below. We also call this device cluster <em>the cluster mesh</em>.</p>
<figure class="align-center" id="id6">
<span id="cluster-mesh"></span><a class="reference internal image-reference" href="../_images/cluster-mesh.png"><img alt="../_images/cluster-mesh.png" src="../_images/cluster-mesh.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-text">Figure 2: an M x N cluster mesh.</span><a class="headerlink" href="#id6" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="device-mesh">
<h4>Device mesh<a class="headerlink" href="#device-mesh" title="Permalink to this headline"></a></h4>
<p>Alpa’s <a class="reference internal" href="#inter-op-pass"><span class="std std-ref">inter-op compilation pass</span></a> will slice the cluster mesh into multiple groups of devices. Each group might contain a number of devices
with high communication bandwidth, such as <a class="reference external" href="https://www.nvidia.com/en-us/data-center/nvlink/">NVIDIA NVLink</a>. We call each group of devices a device mesh.
<a class="reference internal" href="#cluster-mesh"><span class="std std-ref">Figure 2</span></a> shows how a cluster mesh is sliced into 4 device meshes.</p>
</section>
<section id="worker">
<h4>Worker<a class="headerlink" href="#worker" title="Permalink to this headline"></a></h4>
<p>Each device mesh might consist of partial or full devices from a single node or from multiple nodes. Alpa uses a worker to manage multiple devices from a node; hence a device mesh might contain multiple workers, each mapping to a process that manages multiple devices on a node.
For example, <a class="reference internal" href="#mesh-worker"><span class="std std-ref">Figure 3</span></a> shows a mesh, consisted of 2 workers, and each worker manages 4 devices.
The workers are implemented as <a class="reference external" href="https://github.com/alpa-projects/alpa/blob/main/alpa/device_mesh.py">Ray actors</a>.</p>
<figure class="align-center" id="id7">
<span id="mesh-worker"></span><a class="reference internal image-reference" href="../_images/mesh-worker.png"><img alt="../_images/mesh-worker.png" src="../_images/mesh-worker.png" style="width: 350px;" /></a>
<figcaption>
<p><span class="caption-text">Figure 3: A mesh is consisted of multiple workers managing devices.</span><a class="headerlink" href="#id7" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="stage">
<h4>Stage<a class="headerlink" href="#stage" title="Permalink to this headline"></a></h4>
<p>Alpa slices the input computational graph into multiple, adjacent subgraphs. We call each subgraph a stage.</p>
</section>
<section id="resharding">
<h4>Resharding<a class="headerlink" href="#resharding" title="Permalink to this headline"></a></h4>
<p># TODO</p>
</section>
</section>
<section id="compilation-passes">
<h3>Compilation Passes<a class="headerlink" href="#compilation-passes" title="Permalink to this headline"></a></h3>
<p>With the above concepts, we now explain what each compilation pass is exactly doing.</p>
<section id="inter-op-pass">
<span id="id2"></span><h4>Inter-op Pass<a class="headerlink" href="#inter-op-pass" title="Permalink to this headline"></a></h4>
<p>Inter-op pass slices the computational graph into multiple stages and the cluster mesh into multiple smaller device meshes; it then assigns each stage to a mesh.
Alpa generates the slicing and assignment scheme optimally using a dynamic programming algorithm to minimize the inter-op parallel execution latency.</p>
</section>
<section id="intra-op-pass">
<h4>Intra-op pass<a class="headerlink" href="#intra-op-pass" title="Permalink to this headline"></a></h4>
<p>Intra-op pass looks at each &lt;stage, mesh&gt; pair generated by the inter-op pass, and generates the optimal intra-op parallelism execution plan for this stage to run on its assigned mesh.</p>
</section>
<section id="runtime-orchestratoin-pass">
<h4>Runtime Orchestratoin pass<a class="headerlink" href="#runtime-orchestratoin-pass" title="Permalink to this headline"></a></h4>
<p>The runtime orchestration pass looks at the pairs of stages and meshes generated by the inter-op pass, and the intra-op parallelism strategy generated for each &lt;stage, mesh&gt; pair by the intra-op pass.
It analyzes their data dependency, and tries to fullfills some requirements before runtime. These requirements include:</p>
<ul class="simple">
<li><p><strong>Communication</strong>: sending a tensor from a stage to its next stage. When the two stages have different intra-op parallelism execution plan, the tensor might be sharded differently on two meshes.
In that case, cross-mesh resharding is required. Alpa’s runtime orchestration pass will try to generate the optimal scheme on how to communicate the tensors between two meshes.</p></li>
<li><p><strong>Scheduling</strong>: Alpa’s runtime will also compile and generate static scheduling instructions for pipelined execution of all stages, to minimize scheduling overheads at Runtime.</p></li>
</ul>
<p>These three compilation passes are implemented on top of <a class="reference external" href="https://www.tensorflow.org/xla">XLA</a> and <a class="reference external" href="https://arxiv.org/pdf/2105.04663.pdf">GSPMD</a>.
Despite the compilation passes for distributed execution, <a class="reference external" href="https://www.tensorflow.org/xla">XLA</a> and <a class="reference external" href="https://arxiv.org/pdf/2105.04663.pdf">GSPMD</a> additionally perform some other necessary optimizations to improve the single-device execution performance.</p>
</section>
</section>
</section>
<section id="runtime">
<h2>Runtime<a class="headerlink" href="#runtime" title="Permalink to this headline"></a></h2>
<p>Alpa implements a <a class="reference external" href="https://github.com/alpa-projects/alpa/blob/main/alpa/pipeline_parallel/decentralized_distributed_runtime.py">runtime</a> to orchestrate the inter-op parallel execution of different stages on these meshes.
For each stage, Alpa uses the GSPMD runtime to parallelize its execution on its assigned device mesh, following the intra-op parallelism execution plan generated by the intra-op pass.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../benchmark/benchmark.html" class="btn btn-neutral float-left" title="Performance Benchmark" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="alpa_compiler_walk_through.html" class="btn btn-neutral float-right" title="Alpa Compiler Walk-Through" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Alpa Developers.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>