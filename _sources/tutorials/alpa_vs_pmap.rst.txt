
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/alpa_vs_pmap.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_tutorials_alpa_vs_pmap.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_alpa_vs_pmap.py:


Difference between alpa.parallelize and jax.pmap
================================================

The most common tool for parallelization or distributed computing in jax is
`pmap <https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap>`_.
With several lines of code change, we can use ``pmap`` for data parallel
training. However, we cannot use ``pmap`` for model parallel training,
which is required for training large models with billions of parameters.

On the contrary, ``alpa.parallelize`` supports both data parallelism and
model parallelism in an automatic way. ``alpa.parallelize`` analyzes the
jax computational graph and picks the best strategy.
If data parallelism is more suitable, ``alpa.parallelize`` achieves the same
performance as ``pmap`` but with less code change.
If model parallelism is more suitable, ``alpa.parallelize`` achieves better performance
and uses less memory than ``pmap``.

In this tutorial, we are going to compare ``alpa.parallelize`` and ``pmap`` on two
workloads. A more detailed comparison among ``alpa.parallelize``, ``pmap``, and ``xmap``
is also attached at the end of the article.

.. GENERATED FROM PYTHON SOURCE LINES 25-27

When data parallelism is prefered
---------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 27-30

.. code-block:: default


    # TODO


.. GENERATED FROM PYTHON SOURCE LINES 31-33

When model parallelism is prefered
----------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 33-36

.. code-block:: default


    # TODO


.. GENERATED FROM PYTHON SOURCE LINES 37-59

Comparing ``alpa.parallelize``, ``pmap``, and ``xmap``
------------------------------------------------------
Besides ``pmap``, jax also provides
`xmap <https://jax.readthedocs.io/en/latest/notebooks/xmap_tutorial.html>`_
for more advanced parallelization.
The table below compares the features of ``alpa.parallelize``, ``pmap``, and ``xmap``.
In summary, ``alpa.parallelize`` supports more parallelism techniques in a
more automatic way.

================  ================ ==================== ==================== =========
Transformation    Data Parallelism Operator Parallelism Pipeline Parallelism Automated
================  ================ ==================== ==================== =========
alpa.parallelize  yes              yes                  yes                  yes
pmap              yes              no                   no                   no
xmap              yes              yes                  no                   no
================  ================ ==================== ==================== =========

.. note::
  Operator parallelism and pipeline parallelism are two forms of model parallelism.
  Operator parallelism partitions the work in a single operator and assigns them
  to different devices. Pipeline parallelism partitions the computational
  graphs and assigns different operators to different devices.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.000 seconds)


.. _sphx_glr_download_tutorials_alpa_vs_pmap.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: alpa_vs_pmap.py <alpa_vs_pmap.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: alpa_vs_pmap.ipynb <alpa_vs_pmap.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
