{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Distributed Training with Both Intra- and Inter-Operator Parallelism\n\nAlpa can automatically parallelizes jax functions with both intra-operator\nparallelism (e.g. data parallelism, tensor-model parallelism) and inter-operator\nparallelism (e.g. pipeline parallelism). The `getting started guide\n<Getting Started with Alpa>`. focuses on using Alpa for intra-operator\nparallelism.\n\nIn this tutorial, we show how to use Alpa to parallelize an MLP model with\nboth intra- and inter-operator parallelism. First, we show how to use Alpa\nto manually assign stages for inter-operator parallelism. Then we show how\nto use Alpa to automate this process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries and Initialize Environment\nWe first import the required libraries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import alpa\nfrom alpa.testing import assert_allclose\nimport copy\nfrom flax import linen as nn\nfrom flax.training.train_state import TrainState\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nimport optax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Besides alpa and jax related libraries, we also import `ray <https://docs.\nray.io/>`_ and start (or connect to) a ray cluster. We use ray to manage the\ndevices in the distributed cluster in alpa.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import ray\n\nray.init()\n\n# Alternatively, you can use the following command to connect to an existing\n# ray cluster.\n# ray.init(address=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In alpa, the actual computation of a computational graph is executed on ray\nactors. Therefore, we force the driver process to use the CPU to avoid it\nfrom occupying the GPU memory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "jax.config.update('jax_platform_name', 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train an MLP on a Single Device\nIn this tutorial, we use a toy dataset to train an MLP model.\nSpecifically, we use the model to fit the function: $y = Wx + b$.\nNote that now this model is being executed on CPU because we force the driver\nprocess to use the CPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n    hidden_dim: int\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(features=self.hidden_dim * 4)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=self.hidden_dim)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=self.hidden_dim * 4)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=self.hidden_dim)(x)\n        x = nn.relu(x)\n        return x\n\ndim = 2048\nbatch_size = 2048\n\n# Generate ground truth W and b\nrngkey = jax.random.PRNGKey(0)\nk1, k2 = random.split(rngkey)\nW = random.normal(k1, (dim, dim))\nb = random.normal(k2, (dim,))\n\n# Generate the training data\nksample, knoise = random.split(k1)\nx = random.normal(ksample, (batch_size, dim))\ny = (x @ W + b) + 0.1 * random.normal(knoise, (batch_size, dim))\n\n# Initialize a train state, which includes the model paramter and optimizer state.\nmodel = MLPModel(hidden_dim=dim)\nparams = model.init(rngkey, x)\ntx = optax.adam(learning_rate=1e-3)\nstate = TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n\n# Define training step\ndef train_step(state, batch):\n    def loss_func(params):\n        out = model.apply(params, batch[\"x\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        return loss\n\n    grads = jax.grad(loss_func)(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\nbatch = {\"x\": x, \"y\": y}\nexpected_state = train_step(state, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manual Inter-Operator Parallelism with Alpa\nTo manually assign stages for inter-operator parallelism, we can use the\n``alpa.mark_pipeline`` function to mark the start and end of each pipeline stage,\nand use the ``@alpa.manual_layer_construction`` decorator to indicate that we\nare manually assigning stages. Note that all the pipeline stages are also\nautomatically parallelized by the intra-operator parallel pass.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set the number of microbatches for pipeline parallelism.\nnum_micro_batches = 16\n\n# Initialize the alpa device cluster.\ndevice_cluster = alpa.DeviceCluster()\ndevices = device_cluster.get_virtual_physical_mesh()\n\n# Set the parallel strategy to \"pipeshard_parallel\" to enable both inter- and intra-\n# operator parallelism.\nalpa.set_parallelize_options(\n    devices=devices, strategy=\"pipeshard_parallel\",\n    num_micro_batches=num_micro_batches)\n\n# Define the manually parallelized model with pipeline markers.\nclass ManualIntraMLPModel(nn.Module):\n    hidden_dim: int\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(features=self.hidden_dim * 4)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=self.hidden_dim)(x)\n        x = nn.relu(x)\n        # Mark the end of the 0th pipeline stage and the start of the 1st\n        # pipeline stage. the start marker of the 0th stage and the end\n        # marker of the 1st stage are marked in the train_step below.\n        alpa.mark_pipeline(name='0', mark_type='end')\n        alpa.mark_pipeline(name='1', mark_type='start')\n        x = nn.Dense(features=self.hidden_dim * 4)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=self.hidden_dim)(x)\n        x = nn.relu(x)\n        return x\n\n# Initialize the train state with the same parameters as the single-device model.\nmanual_inter_model = ManualIntraMLPModel(hidden_dim=dim)\nmanual_inter_state = TrainState.create(apply_fn=manual_inter_model.apply,\n                                       params=copy.deepcopy(params), tx=tx)\n\n# Define the training step with manually parallelized pipeline stages.\n@alpa.parallelize\ndef manual_inter_train_step(state, batch):\n    # Indicate that we are manually assigning pipeline stages.\n    @alpa.manual_layer_construction\n    def loss_func(params):\n        # Mark the start of the 0th pipeline stage.\n        alpa.mark_pipeline(name='0', mark_type='start')\n        out = state.apply_fn(params, batch[\"x\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        # Mark the end of the 1st pipeline stage.\n        alpa.mark_pipeline(name='1', mark_type='end')\n        return loss\n\n    # We use `alpa.grad` here to seperate the apply gradient stage with the\n    # forward/backward stages in the pipeline. This is necessary to ensure that\n    # the gradient accumulation is correct.\n    grads = alpa.grad(loss_func)(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\nmanual_inter_actual_state = manual_inter_train_step(manual_inter_state, batch)\nassert_allclose(expected_state.params, manual_inter_actual_state.params, atol=5e-3)\n\n# Terminate the alpa device cluster.\nmanual_inter_train_step.get_executable(manual_inter_state, batch).shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Automatic Inter-Operator Parallelism with Alpa\nAlpa also supports automatically partitioning the model into multiple\npipeline stages and assign each pipeline stage a device mesh such that\nthe total execution latency is minimized. Specifically, the automatic\npartitioning algorithm consists of the following steps:\n\n1. **Layer Construction:** In this step, the operators in the model are\n   clustered into ``layers'' based on a graph clustering algorithm. The\n   user needs to specify the total number of layers (i.e. clusters) as\n   a hyperparameter.\n2. **Stage Construction and Mesh Slicing:** In this step, we partition\n   the device cluster (device mesh) to multiple submeshes and assign\n   layers to submeshes to form pipeline stages to minimize the total\n   pipeline execution latency.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create a new cluster class for automatic inter-operator parallelism.\ndevice_cluster = alpa.DeviceCluster()\ndevices = device_cluster.get_virtual_physical_mesh()\n# Set pipeline stage mode to \"auto_gpipe\" to enable automatic inter-operator\n# parallelism with automatic stage slicing and mesh assignment.\nalpa.set_parallelize_options(\n    devices=devices, strategy=\"pipeshard_parallel\", pipeline_stage_mode=\"auto_gpipe\",\n    num_micro_batches=num_micro_batches)\n\n# Define training step with automatic inter-operator parallelism. Note that\n# we reuse the same model and state as the single device case. The only\n# modification required is the two decorators. The stage construction and\n# mesh slicing are performed within the `parallelize` decorator.\n@alpa.parallelize\ndef auto_inter_train_step(state, batch):\n    # Indicate that we use automatic layer construction. The `layer_num` here\n    # is a hyperparameter to control how many layers we get from the\n    # layer construction algorithm.\n    @alpa.automatic_layer_construction(layer_num=2)\n    def loss_func(params):\n        out = state.apply_fn(params, batch[\"x\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        return loss\n\n    # Again, we use `alpa.grad` here to seperate the apply gradient stage with\n    # the forward/backward stages in the pipeline.\n    grads = alpa.grad(loss_func)(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\nauto_inter_actual_state = auto_inter_train_step(state, batch)\nassert_allclose(expected_state.params, auto_inter_actual_state.params, atol=5e-3)\n\nauto_inter_train_step.get_executable(state, batch).shutdown()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}