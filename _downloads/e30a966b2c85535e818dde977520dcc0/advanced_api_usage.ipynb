{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Advanced API Usage\n\nThis page will cover some more advanced examples of Alpa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first import libraries and create example model and train step functions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import flax.linen as nn\nimport jax\nimport jax.numpy as jnp\nimport ray\nimport optax\n\nimport alpa\nfrom alpa import global_config, parallelize\nfrom alpa.device_mesh import DeviceCluster\nfrom alpa.model.bert_model import BertConfig, FlaxBertLayer\nfrom alpa.model.model_util import TrainState\nfrom alpa.util import count_communication_primitives, get_ray_namespace_str\n\n# launch the cluster\nray.init()\ncluster = DeviceCluster()\nglobal_config.devices = cluster.get_physical_mesh()\n\n# define consts\nbatch_size = 64\nseq_len = 512\nhidden_size = 512\nnum_heads = 4\nn_layers = 4\n\n\n# Define model, train state and train step\nclass BertLayerModel(nn.Module):\n    config: BertConfig\n    dtype: jnp.dtype = jnp.float32\n\n    def setup(self):\n        self.layers = [\n            FlaxBertLayer(config=self.config, dtype=self.dtype)\n            for _ in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(self, x, attention_mask):\n        for i, layer in enumerate(self.layers):\n            layer_outputs = layer(x, attention_mask)\n            x = layer_outputs[0]\n        return x\n\n\ndef create_train_state(rngkey, model, inputs):\n    params = model.init(rngkey, *inputs)\n    tx = optax.adam(learning_rate=1e-2)\n    state = TrainState.create(apply_fn=model.apply,\n                              params=params,\n                              tx=tx,\n                              dynamic_scale=None)\n    return state\n\n\nrngkey = jax.random.PRNGKey(0)\nx = jax.random.normal(rngkey, (batch_size, seq_len, hidden_size))\ny = jax.random.normal(rngkey, (batch_size, seq_len, hidden_size))\nattention_mask = jnp.ones((batch_size, seq_len), dtype=jnp.float32)\nbatch = {'x': x, 'y': y, \"attention_mask\": attention_mask}\nbert_config = BertConfig(hidden_size=hidden_size,\n                         intermediate_size=hidden_size * 4,\n                         num_attention_heads=num_heads,\n                         num_hidden_layers=n_layers)\nmodel = BertLayerModel(config=bert_config)\nstate = create_train_state(rngkey, model, [x, attention_mask])\n\n\ndef train_step(state, batch):\n\n    def loss_func(params):\n        out = state.apply_fn(params, batch[\"x\"], batch[\"attention_mask\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        return loss\n\n    grads = jax.grad(loss_func)(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\n\n# define test utils\ndef print_hlo_communication_stats(hlo_text):\n    (n_total, n_all_reduce, n_all_gather, n_reduce_scatter,\n     n_all_to_all) = count_communication_primitives(hlo_text)\n\n    print(f\"#total: {n_total}, #all-reduce: {n_all_reduce}, \"\n          f\"#all-gather: {n_all_gather}, #reduce-scatter: {n_reduce_scatter}, \"\n          f\"#all-to-all: {n_all_to_all}\")\n\n\ndef reset_state():\n    global state\n    state = create_train_state(rngkey, model, [x, attention_mask])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Auto-Sharding Options\n\nAutoShardingOption is designed to control the inter-operator parallelism more precisely.\n\n### Control specific collective primitive\n\nSome primitive is not well-supported on specific platforms(e.g. may cause deadlock).\nIn case of that, they should be excluded in auto-sharding's optimization space.\nWe control this by some auto-sharding options.\n\nIn some cases, an allreduce can be replaced by a reduce-scatter first,\nand an all-gather later. The two has the same communication, but reduce-scatter\nmay readuce the peak memory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "as_option = global_config.default_autosharding_option\nas_option_backup = as_option.backup()\n\nas_option.prefer_reduce_scatter = True\nexecutable = parallelize(train_step).get_executable(state, batch)\nprint_hlo_communication_stats(executable.get_hlo_text())\n\n# create new state to avoid jit\nas_option.prefer_reduce_scatter = False\nstate = create_train_state(rngkey, model, [x, attention_mask])\nexecutable = parallelize(train_step).get_executable(state, batch)\nprint_hlo_communication_stats(executable.get_hlo_text())\n\nas_option.restore(as_option_backup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Force to use data parallel\n\nAlpa can forcibly generates data parallel solution, or map a specific\nmesh dimension to the batch dimension.\n\nWith force_batch_dim_to_mesh_dim, Alpa forcibly maps the given logical mesh\ndimension (0 or 1) to batch dimension inferred in auto-sharding.\nIf the option's value is None, but the two dimensions of the logical mesh is\nlarger than 1, Alpa still forcibly maps the first logical mesh dimension to\nbatch dimension.\n\nWith force_data_parallel, Alpa sets the first dimension larger than 1 to the force_batch_dim_to_mesh_dim value.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Default mesh shape: (num_host,num_device)=(1,4)\n\nas_option.force_batch_dim_to_mesh_dim = 0\nreset_state()\nexecutable = parallelize(train_step).get_executable(state, batch)\nprint_hlo_communication_stats(executable.get_hlo_text())\n# The above uses model parallel\n\nas_option.force_batch_dim_to_mesh_dim = 1\nreset_state()\nexecutable = parallelize(train_step).get_executable(state, batch)\nprint_hlo_communication_stats(executable.get_hlo_text())\n# The above uses data parallel\n\nas_option.restore(as_option_backup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specify inter-operator parallelism strategy\n\nWe can specify inter-operator parallelism config with global_config.\nTo start with, we first set parallel strategy to 3d parallel and use alpa's grad decorator:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "global_config.devices.shutdown()\nglobal_config.strategy = \"pipeshard_parallel\"\nglobal_config.devices = cluster.get_virtual_physical_mesh()\n\n\ndef train_step(state, batch):\n\n    def loss_func(params):\n        out = state.apply_fn(params, batch[\"x\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        return loss\n\n    # modify the grad decorator here\n    grads = alpa.grad(loss_func)(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\n\ndef profile_and_pp_pipeshard_stats(executable):\n    pipeshard_stats = executable.profile_all_executables()\n    print(\"All stages' stats in form of (time, memory)\")\n    for mesh_idx, mesh_stats in enumerate(pipeshard_stats):\n        output_str = \"\"\n        for stat in mesh_stats.values():\n            output_str += f\"({stat[0]:.3f}s,{stat[1]:.2f}GB),\"\n        print(f\"mesh {mesh_idx}:\" + output_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Specify layer clustering\n\nLayer cluster forms a number of JaxprEqns (atom in JAX IR) into the same layer.\nWe can also manually assign layers using the pipeline marker.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from alpa import mark_pipeline, manual_layer_construction\n\n\nclass UnequalManualLayerBertLayerModel(nn.Module):\n    config: BertConfig\n    dtype: jnp.dtype = jnp.float32\n    manual_pipeline_layer: bool = True\n\n    def setup(self):\n        self.layers = [\n            FlaxBertLayer(config=self.config, dtype=self.dtype)\n            for _ in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(self, x, attention_mask):\n        for i, layer in enumerate(self.layers):\n            # Add the pipeline start marker here\n            if i < 2:\n                mark_pipeline(name=str(i), mark_type='start')\n            layer_outputs = layer(x, attention_mask)\n            x = layer_outputs[0]\n            # Add the pipeline end marker here\n            if i == 0 or i == self.config.num_hidden_layers - 1:\n                mark_pipeline(name=str(i), mark_type='end')\n        return x\n\n\ndef train_step(state, batch):\n    # Add the manual layer construction decorator here\n    @manual_layer_construction(lift_markers=True)\n    def loss_func(params):\n        out = state.apply_fn(params, batch[\"x\"], batch[\"attention_mask\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        return loss\n\n    grads = alpa.grad(loss_func)(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\n\nmodel = UnequalManualLayerBertLayerModel(config=bert_config)\nstate = create_train_state(rngkey, model, [x, attention_mask])\n\nexecutable = parallelize(train_step).get_executable(state, batch)\nprofile_and_pp_pipeshard_stats(executable)\n\nexecutable.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code above creates a model with four bert layers, then split them into\ntwo alpa layers. With default setting, each layer maps a pipeline stage and\neach stage use the same submesh. As we split between the first bert layer and\nthe other three layers, the memory consumption of the first stage is\napproximately third of the second's.\n\nIn manual layer construction, each instruction in the forward computation\nshould between a pipeline start marker and its corresponding pipeline end\nmarker. When using the manual pipeline marker, the loss function should be\ndecorated by the manual_layer_construction mark.\n\nFor simplicity, manual_layer_construction provides a lift_marker option.\nIf it is turned on, the first and last pipeline marker are automatically\nmoved to the first and last JaxprEqn.\n\n### Specify stage construction\n\nStage construction merges layers into stages and assigns devices to each stage\nwith a logical mesh shape. Here we manually give the stage construction plan\nwith options in global_config.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class EqualManualLayerBertLayerModel(nn.Module):\n    config: BertConfig\n    dtype: jnp.dtype = jnp.float32\n    manual_pipeline_layer: bool = True\n\n    def setup(self):\n        self.layers = [\n            FlaxBertLayer(config=self.config, dtype=self.dtype)\n            for _ in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(self, x, attention_mask):\n        for i, layer in enumerate(self.layers):\n            # Add the pipeline start marker here\n            mark_pipeline(name=str(i), mark_type='start')\n            layer_outputs = layer(x, attention_mask)\n            x = layer_outputs[0]\n            # Add the pipeline end marker here\n            mark_pipeline(name=str(i), mark_type='end')\n        return x\n\n\nmodel = EqualManualLayerBertLayerModel(config=bert_config)\nstate = create_train_state(rngkey, model, [x, attention_mask])\n\nglobal_config_backup = global_config.backup()\n\n# turn on manual stage plan\nglobal_config.pipeline_stage_mode = \"manual_gpipe\"\n# Layer-stage mapping\nglobal_config.forward_stage_layer_ids = [[0], [1], [2, 3]]\n# Physical mesh shape of each stage\nglobal_config.sub_physical_mesh_shapes = [(1, 1), (1, 1), (1, 2)]\n# Logical mesh shape of each stage\nglobal_config.sub_logical_mesh_shapes = [(1, 1), (1, 1), (2, 1)]\n# auto sharding option of each stage\nglobal_config.submesh_autosharding_option_dicts = [{}, {}, {}]\nexecutable = parallelize(train_step).get_executable(state, batch)\nprofile_and_pp_pipeshard_stats(executable)\n\nexecutable.shutdown()\nglobal_config.restore(global_config_backup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rematerialization with layer construction\n\nWe provide a layer-based rematerialization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = EqualManualLayerBertLayerModel(config=bert_config)\nstate = create_train_state(rngkey, model, [x, attention_mask])\n\n\ndef get_train_step(remat_layer):\n\n    def train_step(state, batch):\n\n        # Set remat_layer in manual layer construction decorator here.\n        # The same is true for automatic layer construction decorator.\n        @manual_layer_construction(lift_markers=True, remat_layer=remat_layer)\n        def loss_func(params):\n            out = state.apply_fn(params, batch[\"x\"], batch[\"attention_mask\"])\n            loss = jnp.mean((out - batch[\"y\"])**2)\n            return loss\n\n        grads = alpa.grad(loss_func)(state.params)\n        new_state = state.apply_gradients(grads=grads)\n        return new_state\n\n    return train_step\n\n\nprint(\">>>>> With remat\")\nexecutable = parallelize(get_train_step(True)).get_executable(state, batch)\nprofile_and_pp_pipeshard_stats(executable)\nexecutable.shutdown()\nreset_state()\nprint(\">>>>> Without remat\")\nexecutable = parallelize(get_train_step(False)).get_executable(state, batch)\nprofile_and_pp_pipeshard_stats(executable)\nexecutable.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The peak memory is significantly smaller when remat_layer is turned on.\n\nMoreover, we can remat at a fine-grained level, then do parallel at a relatively\ncoarse-grained level. The example below remat at each Bert Layer, but do\ninter-operator parallelization for each two Bert Layers\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from alpa import automatic_remat, automatic_layer_construction\n\nmodel = BertLayerModel(config=bert_config)\n\n\ndef get_train_step(remat_layer):\n\n    def train_step(state, batch):\n\n        def loss_func(params):\n            out = state.apply_fn(params, batch[\"x\"], batch[\"attention_mask\"])\n            loss = jnp.mean((out - batch[\"y\"])**2)\n            return loss\n\n        # Split the forward into 4 parts for remat\n        if remat_layer:\n            loss_func = automatic_remat(loss_func, layer_num=4)\n        # Split the forward(remat-marked) into 2 parts for inter-operator parallel\n        loss_func = automatic_layer_construction(loss_func, layer_num=2)\n        grads = alpa.grad(loss_func)(state.params)\n        new_state = state.apply_gradients(grads=grads)\n        return new_state\n\n    return train_step\n\n\nprint(\">>>>> With remat\")\nstate = create_train_state(rngkey, model, [x, attention_mask])\nexecutable = parallelize(get_train_step(True)).get_executable(state, batch)\nprofile_and_pp_pipeshard_stats(executable)\nexecutable.shutdown()\nreset_state()\nprint(\">>>>> Without remat\")\nexecutable = parallelize(get_train_step(False)).get_executable(state, batch)\nprofile_and_pp_pipeshard_stats(executable)\nexecutable.shutdown()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}