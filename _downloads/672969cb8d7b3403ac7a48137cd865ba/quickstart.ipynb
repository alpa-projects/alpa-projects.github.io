{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Alpa Quickstart\n\nAlpa is built on top of a tensor computation framework `Jax <https://jax.readthedocs.io/en/latest/index.html>`_ .\nAlpa can automatically parallelize jax functions and runs them on a distributed cluster.\nAlpa analyses the computational graph and generates a distributed execution plan \ntailored for the computational graph and target cluster.\nThe generated execution plan can combine state-of-the-art distributed training techniques\nincluding data parallelism, operator parallelism, and pipeline parallelism.\n\nAlpa provides a simple API ``alpa.parallelize`` and automatically generates the best execution\nplan by solving optimization problems. Therefore, you can efficiently scale your jax computation\non a distributed cluster, without any expertise in distributed computing.\n\nIn this tutorial, we show the usage of Alpa with an MLP example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries\nWe first import the required libraries.\nFlax and optax are libraries on top of jax for training neural networks.\nAlthough we use these libraries in this example, Alpa works on jax's and XLA's internal\nintermediate representations and does not depend on any specific high-level libraries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from functools import partial\n\nimport alpa\nfrom alpa.testing import assert_allclose\nfrom flax import linen as nn\nfrom flax.training.train_state import TrainState\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nimport numpy as np\nimport optax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train an MLP on a Single Device\nTo begin with, we implement the model and training loop on a single device. We will\nparallelize it later. We train an MLP to learn a function y = Wx + b.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n    hidden_dim: int\n    num_layers: int\n\n    @nn.compact\n    def __call__(self, x):\n        for i in range(self.num_layers):\n            if i % 2 == 0:\n                x = nn.Dense(features=self.hidden_dim * 4)(x)\n            else:\n                x = nn.Dense(features=self.hidden_dim)(x)\n            x = nn.relu(x)\n        return x\n\ndim = 2048\nbatch_size = 2048\nnum_layers = 10\n\n# Generate ground truth W and b\nrngkey = jax.random.PRNGKey(0)\nk1, k2 = random.split(rngkey)\nW = random.normal(k1, (dim, dim), jnp.float32)\nb = random.normal(k2, (dim,), jnp.float32)\n\n# Generate the training data\nksample, knoise = random.split(k1)\nx = random.normal(ksample, (batch_size, dim), jnp.float32)\ny = (x @ W + b) + 0.1 * random.normal(knoise, (batch_size, dim))\n\n# Initialize a train state, which includes the model paramter and optimizer state.\nmodel = MLPModel(hidden_dim=dim, num_layers=num_layers)\nparams = model.init(rngkey, x)\ntx = optax.adam(learning_rate=1e-3)\nstate = TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n\n# Define the training function and execute one step\ndef train_step(state, batch):\n    def loss_func(params):\n        out = state.apply_fn(params, batch[\"x\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        return loss\n\n    grads = jax.grad(loss_func)(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\nbatch = {\"x\": x, \"y\": y}\nexpected_state = train_step(state, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Auto-parallelization with ``alpa.parallelize``\nAlpa provides a transformation ``alpa.parallelize`` to parallelize a jax function.\n``alpa.parallelize`` is similar to ``jax.jit`` . ``jax.jit`` compiles a jax\nfunction for a single device, while ``alpa.parallelize`` compiles a jax function\nfor a distributed device cluster.\nYou may know that jax has some built-in transformations for parallelization,\nsuch as ``pmap``, ``pjit``, and ``xmap``. However, these transformations are not\nfully automatic, because they require users to manually specify the parallelization\nstrategies such as parallelization axes and device mapping schemes. You also need to\nmanually call communication primitives such as ``lax.pmean`` and ``lax.all_gather``,\nwhich is nontrivial if you want to do advanced model parallelization.\nUnlike these transformations, ``alpa.parallelize`` can do all things automatically for\nyou. ``alpa.parallelize`` finds the best parallelization strategy for the given jax\nfunction and does the code tranformation. You only need to write the code as if you are\nwriting for a single device.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define the training step. The body of this function is the same as the\n# ``train_step`` above. The only difference is to decorate it with\n# ``alpa.paralellize``.\n\n@alpa.parallelize\ndef alpa_train_step(state, batch):\n    def loss_func(params):\n        out = model.apply(params, batch[\"x\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        return loss\n\n    grads = jax.grad(loss_func)(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\n# Test correctness\nactual_state = alpa_train_step(state, batch)\nassert_allclose(expected_state.params, actual_state.params, atol=5e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After being decorated by ``alpa.parallelize``, the function can still take numpy\narrays or jax arrays as inputs. The function will first distribute the input\narrays into correct devices according to the parallelization strategy and then\nexecute the function distributedly. The returned result arrays are also\nstored distributedly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Input parameter type:\", type(state.params[\"params\"][\"Dense_0\"][\"kernel\"]))\nprint(\"Output parameter type:\", type(actual_state.params[\"params\"][\"Dense_0\"][\"kernel\"]))\n\n# Get one copy\nkernel_np = np.array(actual_state.params[\"params\"][\"Dense_0\"][\"kernel\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execution Speed Comparison\nBy parallelizing a jax function, we can accelerate the computation and reduce\nthe memory usage per GPU, so we can train large models faster.\nWe benchmark the execution speed of ``jax.jit`` and ``alpa.parallelize``\non a 8-GPU machine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "state = actual_state  # We need this assignment because the original `state` is \"donated\" and freed.\nfrom alpa.util import benchmark_func\n\n# Benchmark serial execution with jax.jit\njit_train_step = jax.jit(train_step, donate_argnums=(0,))\n\ndef sync_func():\n    jax.local_devices()[0].synchronize_all_activity()\n\ndef serial_execution():\n    global state\n    state = jit_train_step(state, batch)\n\ncosts = benchmark_func(serial_execution, sync_func, warmup=5, number=10, repeat=5) * 1e3\n\nprint(f\"Serial execution time. Mean: {np.mean(costs):.2f} ms, Std: {np.std(costs):.2f} ms\")\n\n# Benchmark parallel execution\n# We distribute arguments in advance for the benchmarking purpose\nstate, batch = alpa_train_step.preshard_dynamic_args(state, batch)\n\ndef parallel_execution():\n    global state\n    state = alpa_train_step(state, batch)\n\ncosts = benchmark_func(parallel_execution, sync_func, warmup=5, number=10, repeat=5) * 1e3\n\nprint(f\"Parallel execution time. Mean: {np.mean(costs):.2f} ms, Std: {np.std(costs):.2f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory Usage Comparison\nWe can also compare the memory usage per GPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "GB = 1024 ** 3\n\nexecutable = jit_train_step.lower(state, batch).compile().runtime_executable()\nprint(f\"Serial execution per GPU memory usage: {executable.total_allocation_size() / GB:.2f} GB\")\n\nexecutable = alpa_train_step.get_executable(state, batch)\nprint(f\"Parallel execution per GPU memory usage: {executable.get_total_allocation_size() / GB:.2f} GB\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}