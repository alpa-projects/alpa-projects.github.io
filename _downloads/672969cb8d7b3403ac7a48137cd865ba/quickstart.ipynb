{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Alpa Quickstart\n\nAlpa is built on top of a tensor computation framework `Jax <https://jax.readthedocs.io/en/latest/index.html>`_ .\nAlpa can automatically parallelize jax functions and runs them on a distributed cluster.\nAlpa analyses the computational graph and generates a distributed execution plan\ntailored for the computational graph and target cluster.\nThe generated execution plan can combine state-of-the-art distributed training techniques\nincluding data parallelism, operator parallelism, and pipeline parallelism.\n\nAlpa provides a simple API ``alpa.parallelize`` and automatically generates the best execution\nplan by solving optimization problems. Therefore, you can efficiently scale your jax computation\non a distributed cluster, without any expertise in distributed computing.\n\nIn this tutorial, we show the usage of Alpa with an MLP example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries\nWe first import the required libraries.\nFlax and optax are libraries on top of jax for training neural networks.\nAlthough we use these libraries in this example, Alpa works on jax's and XLA's internal\nintermediate representations and does not depend on any specific high-level libraries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from functools import partial\n\nimport alpa\nfrom alpa.testing import assert_allclose\nfrom flax import linen as nn\nfrom flax.training.train_state import TrainState\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nimport numpy as np\nimport optax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train an MLP on a Single Device\nTo begin with, we implement the model and training loop on a single device. We will\nparallelize it later. We train an MLP to learn a function y = Wx + b.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n    hidden_dim: int\n    num_layers: int\n\n    @nn.compact\n    def __call__(self, x):\n        for i in range(self.num_layers):\n            if i % 2 == 0:\n                x = nn.Dense(features=self.hidden_dim * 4)(x)\n            else:\n                x = nn.Dense(features=self.hidden_dim)(x)\n            x = nn.relu(x)\n        return x\n\ndim = 2048\nbatch_size = 2048\nnum_layers = 10\n\n# Generate ground truth W and b\nrngkey = jax.random.PRNGKey(0)\nk1, k2 = random.split(rngkey)\nW = random.normal(k1, (dim, dim))\nb = random.normal(k2, (dim,))\n\n# Generate the training data\nksample, knoise = random.split(k1)\nx = random.normal(ksample, (batch_size, dim))\ny = (x @ W + b) + 0.1 * random.normal(knoise, (batch_size, dim))\n\n# Initialize a train state, which includes the model paramter and optimizer state.\nmodel = MLPModel(hidden_dim=dim, num_layers=num_layers)\nparams = model.init(rngkey, x)\ntx = optax.adam(learning_rate=1e-3)\nstate = TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n\n# Define the training function and execute one step\ndef train_step(state, batch):\n    def loss_func(params):\n        out = state.apply_fn(params, batch[\"x\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        return loss\n\n    grads = jax.grad(loss_func)(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\nbatch = {\"x\": x, \"y\": y}\nexpected_state = train_step(state, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Auto-parallelization with ``alpa.parallelize``\nAlpa provides a transformation ``alpa.parallelize`` to parallelize a jax function.\n``alpa.parallelize`` is similar to ``jax.jit`` . ``jax.jit`` compiles a jax\nfunction for a single device, while ``alpa.parallelize`` compiles a jax function\nfor a distributed device cluster.\nYou may know that jax has some built-in transformations for parallelization,\nsuch as ``pmap``, ``pjit``, and ``xmap``. However, these transformations are not\nfully automatic, because they require users to manually specify the parallelization\nstrategies such as parallelization axes and device mapping schemes. You also need to\nmanually call communication primitives such as ``lax.pmean`` and ``lax.all_gather``,\nwhich is nontrivial if you want to do advanced model parallelization.\nUnlike these transformations, ``alpa.parallelize`` can do all things automatically for\nyou. ``alpa.parallelize`` finds the best parallelization strategy for the given jax\nfunction and does the code tranformation. You only need to write the code as if you are\nwriting for a single device.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define the training step. The body of this function is the same as the\n# ``train_step`` above. The only difference is to decorate it with\n# ``alpa.paralellize``.\n\n@alpa.parallelize\ndef alpa_train_step(state, batch):\n    def loss_func(params):\n        out = state.apply_fn(params, batch[\"x\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        return loss\n\n    grads = jax.grad(loss_func)(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\n# Test correctness\nactual_state = alpa_train_step(state, batch)\nassert_allclose(expected_state.params, actual_state.params, atol=5e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After being decorated by ``alpa.parallelize``, the function can still take numpy\narrays or jax arrays as inputs. The function will first distribute the input\narrays into correct devices according to the parallelization strategy and then\nexecute the function distributedly. The returned result arrays are also\nstored distributedly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Input parameter type:\", type(state.params[\"params\"][\"Dense_0\"][\"kernel\"]))\nprint(\"Output parameter type:\", type(actual_state.params[\"params\"][\"Dense_0\"][\"kernel\"]))\n\n# We can use `np.array` to convert a distributed array back to a numpy array.\nkernel_np = np.array(actual_state.params[\"params\"][\"Dense_0\"][\"kernel\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execution Speed Comparison\nBy parallelizing a jax function, we can accelerate the computation and reduce\nthe memory usage per GPU, so we can train larger models faster.\nWe benchmark the execution speed of ``jax.jit`` and ``alpa.parallelize``\non a 8-GPU machine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "state = actual_state  # We need this assignment because the original `state` is \"donated\" and freed.\nfrom alpa.util import benchmark_func\n\n# Benchmark serial execution with jax.jit\njit_train_step = jax.jit(train_step, donate_argnums=(0,))\n\ndef sync_func():\n    jax.local_devices()[0].synchronize_all_activity()\n\ndef serial_execution():\n    global state\n    state = jit_train_step(state, batch)\n\ncosts = benchmark_func(serial_execution, sync_func, warmup=5, number=10, repeat=5) * 1e3\nprint(f\"Serial execution time. Mean: {np.mean(costs):.2f} ms, Std: {np.std(costs):.2f} ms\")\n\n# Benchmark parallel execution with alpa\n# We distribute arguments in advance for the benchmarking purpose.\nstate, batch = alpa_train_step.preshard_dynamic_args(state, batch)\n\ndef alpa_execution():\n    global state\n    state = alpa_train_step(state, batch)\n\nalpa_costs = benchmark_func(alpa_execution, sync_func, warmup=5, number=10, repeat=5) * 1e3\nprint(f\"Alpa execution time.   Mean: {np.mean(alpa_costs):.2f} ms, Std: {np.std(alpa_costs):.2f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory Usage Comparison\nWe can also compare the memory usage per GPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "GB = 1024 ** 3\n\nexecutable = jit_train_step.lower(state, batch).compile().runtime_executable()\nprint(f\"Serial execution per GPU memory usage: {executable.total_allocation_size() / GB:.2f} GB\")\n\nalpa_executable = alpa_train_step.get_executable(state, batch)\nprint(f\"Alpa execution per GPU memory usage:   {alpa_executable.get_total_allocation_size() / GB:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison against Data Parallelism (or ``jax.pmap``)\nThe most common parallelization technique in deep learning is data parallelism.\nIn jax, we can use ``jax.pmap`` to implement data parallelism.\nHowever, data parallelism only is not enough for training large models due to\nboth memory and communication costs. Here, we use the same model to benchmark the\nexecution speed and memory usage of ``jax.pmap`` on the same 8-GPU machine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@partial(jax.pmap, axis_name=\"batch\")\ndef pmap_train_step(state, batch):\n    def loss_func(params):\n        out = model.apply(params, batch[\"x\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        return loss\n\n    grads = jax.grad(loss_func)(state.params)\n    # all-reduce gradients\n    grads = jax.lax.pmean(grads, axis_name=\"batch\")\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\n# Replicate model and distribute batch\ndevices = jax.local_devices()\nstate = jax.device_put_replicated(state, devices)\ndef shard_batch(x):\n    x = x.reshape((len(devices), -1) + x.shape[1:])\n    return jax.device_put_sharded(list(x), devices)\nbatch = jax.tree_map(shard_batch, batch)\n\n# Benchmark data parallel execution\ndef data_parallel_execution():\n    global state\n    state = pmap_train_step(state, batch)\n\ncosts = benchmark_func(data_parallel_execution, sync_func, warmup=5, number=10, repeat=5) * 1e3\nprint(f\"Data parallel execution time. Mean: {np.mean(costs):.2f} ms, Std: {np.std(costs):.2f} ms\")\nprint(f\"Alpa execution time.          Mean: {np.mean(alpa_costs):.2f} ms, Std: {np.std(alpa_costs):.2f} ms\\n\")\n\nexecutable = pmap_train_step.lower(state, batch).compile().runtime_executable()\nprint(f\"Data parallel execution per GPU memory usage: {executable.total_allocation_size() / GB:.2f} GB\")\nprint(f\"Alpa execution per GPU memory usage:          {alpa_executable.get_total_allocation_size() / GB:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, ``alpa.parallelize`` achieves better execution speed and\nrequires less memory compared with data parallelism.\nThis is because data parallelism only works well if the activation size is much\nlarger than the model size, which is not the case in this benchmark.\nIn contrast, ``alpa.parallelize`` analyzes the computational graph and\nfinds the best parallelization strategy.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}