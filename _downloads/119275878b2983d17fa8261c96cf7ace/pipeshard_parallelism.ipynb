{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Distributed Training with Both Shard and Pipeline Parallelism\n\nAlpa can automatically parallelizes jax functions with both shard\nparallelism, including data parallelism and operator parallelism (a.k.a.\nintra-operator parallelism), and pipeline parallelism (a.k.a. inter-operator\nparallelism). The `getting started guide <Getting Started with Alpa>`\nfocuses on using Alpa for shard parallelism.\n\nIn this tutorial, we show how to use Alpa to parallelize an MLP model with\nboth shard and pipeline parallelism. First, we show how to use Alpa\nto manually assign stages for pipeline parallelism. Then we show how\nto use Alpa to automate this process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries and Initialize Environment\nWe first import the required libraries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import alpa\nfrom alpa.testing import assert_allclose\nimport copy\nfrom flax import linen as nn\nfrom flax.training.train_state import TrainState\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nimport optax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Besides alpa and jax related libraries, we also import `ray <https://docs.\nray.io/>`_ and start (or connect to) a ray cluster. We use ray to manage the\ndevices in the distributed cluster in alpa.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import ray\n\nray.init()\n\n# Alternatively, you can use the following command to connect to an existing\n# ray cluster.\n# ray.init(address=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In alpa, the actual computation of a computational graph is executed on ray\nactors. Therefore, we force the driver process to use the CPU to avoid it\nfrom occupying the GPU memory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "jax.config.update('jax_platform_name', 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train an MLP on a Single Device\nIn this tutorial, we use a toy dataset to train an MLP model.\nSpecifically, we use the model to fit the function: $y = Wx + b$.\nNote that now this model is being executed on CPU because we force the driver\nprocess to use the CPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n    hidden_dim: int\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(features=self.hidden_dim * 4)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=self.hidden_dim)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=self.hidden_dim * 4)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=self.hidden_dim)(x)\n        x = nn.relu(x)\n        return x\n\ndim = 2048\nbatch_size = 2048\n\n# Generate ground truth W and b\nrngkey = jax.random.PRNGKey(0)\nk1, k2 = random.split(rngkey)\nW = random.normal(k1, (dim, dim))\nb = random.normal(k2, (dim,))\n\n# Generate the training data\nksample, knoise = random.split(k1)\nx = random.normal(ksample, (batch_size, dim))\ny = (x @ W + b) + 0.1 * random.normal(knoise, (batch_size, dim))\n\n# Initialize a train state, which includes the model paramter and optimizer\n# state.\nmodel = MLPModel(hidden_dim=dim)\nparams = model.init(rngkey, x)\ntx = optax.adam(learning_rate=1e-3)\nstate = TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n\n# Define training step\ndef train_step(state, batch):\n    def loss_func(params):\n        out = model.apply(params, batch[\"x\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        return loss\n\n    grads = jax.grad(loss_func)(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\nbatch = {\"x\": x, \"y\": y}\nexpected_state = train_step(state, batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manual Pipeline Parallelism with Alpa\nTo manually assign stages for pipeline parallelism, we can use the\n``alpa.mark_pipeline`` function to mark the start and end of each pipeline\nstage, and use the ``@alpa.manual_layer_construction`` decorator to indicate\nthat we are manually assigning stages. Note that each the pipeline stage is\nalso automatically parallelized by the shard parallel pass.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set the number of microbatches for pipeline parallelism.\nnum_micro_batches = 16\n\n# Initialize the alpa device cluster.\ndevice_cluster = alpa.DeviceCluster()\ndevices = device_cluster.get_virtual_physical_mesh()\n\n# Set the parallel strategy to \"pipeshard_parallel\" to enable both pipeline and\n# shard parallelism.\nalpa.set_parallelize_options(\n    devices=devices, strategy=\"pipeshard_parallel\",\n    num_micro_batches=num_micro_batches)\n\n# Define the manually parallelized model with pipeline markers.\nclass ManualPipelineMLPModel(nn.Module):\n    hidden_dim: int\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(features=self.hidden_dim * 4)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=self.hidden_dim)(x)\n        x = nn.relu(x)\n        # Mark the end of the 0th pipeline stage and the start of the 1st\n        # pipeline stage. the start marker of the 0th stage and the end\n        # marker of the 1st stage are marked in the train_step below.\n        alpa.mark_pipeline(name='0', mark_type='end')\n        alpa.mark_pipeline(name='1', mark_type='start')\n        x = nn.Dense(features=self.hidden_dim * 4)(x)\n        x = nn.relu(x)\n        x = nn.Dense(features=self.hidden_dim)(x)\n        x = nn.relu(x)\n        return x\n\n# Initialize the train state with the same parameters as the single-device\n# model.\nmanual_pipeline_model = ManualPipelineMLPModel(hidden_dim=dim)\nmanual_pipeline_state = TrainState.create(apply_fn=manual_pipeline_model.apply,\n                                       params=copy.deepcopy(params), tx=tx)\n\n# Define the training step with manually parallelized pipeline stages.\n@alpa.parallelize\ndef manual_pipeline_train_step(state, batch):\n    # Indicate that we are manually assigning pipeline stages.\n    @alpa.manual_layer_construction\n    def loss_func(params):\n        # Mark the start of the 0th pipeline stage.\n        alpa.mark_pipeline(name='0', mark_type='start')\n        out = state.apply_fn(params, batch[\"x\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        # Mark the end of the 1st pipeline stage.\n        alpa.mark_pipeline(name='1', mark_type='end')\n        return loss\n\n    # We use `alpa.grad` here to seperate the apply gradient stage with the\n    # forward/backward stages in the pipeline. This is necessary to ensure that\n    # the gradient accumulation is correct.\n    grads = alpa.grad(loss_func)(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\nmanual_pipeline_actual_state = manual_pipeline_train_step(manual_pipeline_state,\n                                                          batch)\nassert_allclose(expected_state.params, manual_pipeline_actual_state.params,\n                atol=5e-3)\n\n# Terminate the alpa device cluster.\nmanual_pipeline_train_step.get_executable(manual_pipeline_state,\n                                          batch).shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Automatic Pipeline Parallelism with Alpa\nAlpa also supports automatically partitioning the model into multiple\npipeline stages and assign each pipeline stage a device mesh such that\nthe total execution latency is minimized. Specifically, the automatic\npartitioning algorithm consists of the following steps:\n\n1. **Layer Construction:** In this step, the operators in the model are\n   clustered into ``layers'' based on a graph clustering algorithm. The\n   user needs to specify the total number of layers (i.e. clusters) as\n   a hyperparameter.\n2. **Stage Construction and Mesh Slicing:** In this step, we partition\n   the device cluster (device mesh) to multiple submeshes and assign\n   layers to submeshes to form pipeline stages to minimize the total\n   pipeline execution latency.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create a new cluster class for automatic pipeline parallelism.\ndevice_cluster = alpa.DeviceCluster()\ndevices = device_cluster.get_virtual_physical_mesh()\n# Set pipeline stage mode to \"auto_gpipe\" to enable automatic\n# parallelism with automatic stage slicing and mesh assignment.\nalpa.set_parallelize_options(\n    devices=devices, strategy=\"pipeshard_parallel\",\n    pipeline_stage_mode=\"auto_gpipe\", num_micro_batches=num_micro_batches)\n\n# Define training step with automatic pipeline-operator parallelism. Note that\n# we reuse the same model and state as the single device case. The only\n# modification required is the two decorators. The stage construction and\n# mesh slicing are performed within the `parallelize` decorator.\n@alpa.parallelize\ndef auto_pipeline_train_step(state, batch):\n    # Indicate that we use automatic layer construction. The `layer_num` here\n    # is a hyperparameter to control how many layers we get from the\n    # layer construction algorithm.\n    @alpa.automatic_layer_construction(layer_num=2)\n    def loss_func(params):\n        out = state.apply_fn(params, batch[\"x\"])\n        loss = jnp.mean((out - batch[\"y\"])**2)\n        return loss\n\n    # Again, we use `alpa.grad` here to seperate the apply gradient stage with\n    # the forward/backward stages in the pipeline.\n    grads = alpa.grad(loss_func)(state.params)\n    new_state = state.apply_gradients(grads=grads)\n    return new_state\n\nauto_pipeline_actual_state = auto_pipeline_train_step(state, batch)\nassert_allclose(expected_state.params, auto_pipeline_actual_state.params,\n                atol=5e-3)\n\nauto_pipeline_train_step.get_executable(state, batch).shutdown()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}